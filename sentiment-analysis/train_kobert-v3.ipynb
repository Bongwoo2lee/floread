{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10092</th>\n",
       "      <td>자업자득인거다...</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>예언 목사님이거든!</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44757</th>\n",
       "      <td>친구가 당연한 걸 물어서 친구에게 넌 왜 이렇게 애가 멍청하다고 말했더니 죄책감이 ...</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12238</th>\n",
       "      <td>눈이 오려나? 어, 저거 뭐야?</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>어딜가야될지잘모르겠어요.</td>\n",
       "      <td>불안</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence emotion\n",
       "10092                                         자업자득인거다...      놀람\n",
       "4962                                          예언 목사님이거든!      분노\n",
       "44757  친구가 당연한 걸 물어서 친구에게 넌 왜 이렇게 애가 멍청하다고 말했더니 죄책감이 ...      놀람\n",
       "12238                                  눈이 오려나? 어, 저거 뭐야?      놀람\n",
       "2748                                       어딜가야될지잘모르겠어요.      불안"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋.csv', index_col=0) \n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30418</th>\n",
       "      <td>존경합니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48710</th>\n",
       "      <td>아직 나이가 많지도 않은데 여자라는 이유로 해고당했어. 너무 불행하다고 생각이 들어.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8289</th>\n",
       "      <td>손연재 갑자기 뜨더라</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53785</th>\n",
       "      <td>내 여동생 온대.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27611</th>\n",
       "      <td>도서실에서 혼자 공부하니깐 좀 외롭다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  emotion\n",
       "30418                                            존경합니다        0\n",
       "48710  아직 나이가 많지도 않은데 여자라는 이유로 해고당했어. 너무 불행하다고 생각이 들어.        3\n",
       "8289                                       손연재 갑자기 뜨더라        2\n",
       "53785                                        내 여동생 온대.        5\n",
       "27611                            도서실에서 혼자 공부하니깐 좀 외롭다.        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "# emotions = {'기쁨': 0, '불안': 1, '당황': 2, '슬픔': 3, '분노': 4, '상처': 5} // 감성대화 말뭉치\n",
    "\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\")\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters(KoBERT finetuning 베에스 라인) -> \n",
    "max_len = 64    #베이스라인 64\n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b0c5aecd454d6c8047ff49341274f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.074551582336426 train acc 0.09375\n",
      "epoch 1 batch id 201 loss 1.6258450746536255 train acc 0.23056592039800994\n",
      "epoch 1 batch id 401 loss 1.3003720045089722 train acc 0.3376714463840399\n",
      "epoch 1 batch id 601 loss 1.1646114587783813 train acc 0.40656198003327787\n",
      "epoch 1 batch id 801 loss 1.1231086254119873 train acc 0.45072565543071164\n",
      "epoch 1 batch id 1001 loss 1.217494249343872 train acc 0.4818306693306693\n",
      "epoch 1 batch id 1201 loss 0.9632625579833984 train acc 0.5051259367194005\n",
      "epoch 1 batch id 1401 loss 1.103046178817749 train acc 0.5221047466095646\n",
      "epoch 1 batch id 1601 loss 0.8962392210960388 train acc 0.5346658338538414\n",
      "epoch 1 batch id 1801 loss 1.1236183643341064 train acc 0.5446106329816769\n",
      "epoch 1 batch id 2001 loss 1.302586555480957 train acc 0.5533951774112944\n",
      "epoch 1 batch id 2201 loss 1.280333399772644 train acc 0.5618184915947296\n",
      "epoch 1 batch id 2401 loss 0.9095034599304199 train acc 0.5678883798417326\n",
      "epoch 1 batch id 2601 loss 0.9173089861869812 train acc 0.5750072087658593\n",
      "epoch 1 batch id 2801 loss 0.8827694654464722 train acc 0.5808081935023206\n",
      "epoch 1 batch id 3001 loss 0.851591944694519 train acc 0.58544026991003\n",
      "epoch 1 batch id 3201 loss 0.7139877080917358 train acc 0.5886734614183068\n",
      "epoch 1 batch id 3401 loss 0.8835700750350952 train acc 0.5929230373419583\n",
      "epoch 1 train acc 0.5960276763135712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5905cb5f7d6f420fba3ee1b05daf0378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6608700473092844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1eae2ba3c474a89abf3c28d3015786d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.859921932220459 train acc 0.71875\n",
      "epoch 2 batch id 201 loss 0.9067028760910034 train acc 0.6557835820895522\n",
      "epoch 2 batch id 401 loss 0.8948537111282349 train acc 0.6587437655860349\n",
      "epoch 2 batch id 601 loss 0.7162104249000549 train acc 0.6612416805324459\n",
      "epoch 2 batch id 801 loss 0.8603163361549377 train acc 0.6656523096129837\n",
      "epoch 2 batch id 1001 loss 0.9884093403816223 train acc 0.6677697302697303\n",
      "epoch 2 batch id 1201 loss 0.8871845602989197 train acc 0.6706130308076603\n",
      "epoch 2 batch id 1401 loss 0.8918684124946594 train acc 0.6730906495360457\n",
      "epoch 2 batch id 1601 loss 1.0160996913909912 train acc 0.6759837601499064\n",
      "epoch 2 batch id 1801 loss 1.0455106496810913 train acc 0.6790671848972792\n",
      "epoch 2 batch id 2001 loss 1.1696271896362305 train acc 0.6812843578210894\n",
      "epoch 2 batch id 2201 loss 1.1378229856491089 train acc 0.6840782598818719\n",
      "epoch 2 batch id 2401 loss 0.6163977980613708 train acc 0.6858600583090378\n",
      "epoch 2 batch id 2601 loss 0.8101378083229065 train acc 0.6884131103421761\n",
      "epoch 2 batch id 2801 loss 0.8188526034355164 train acc 0.6903226526240628\n",
      "epoch 2 batch id 3001 loss 0.5668004155158997 train acc 0.6918527157614128\n",
      "epoch 2 batch id 3201 loss 0.4443407952785492 train acc 0.6928596532333646\n",
      "epoch 2 batch id 3401 loss 0.6711865067481995 train acc 0.6949150985004411\n",
      "epoch 2 train acc 0.696410508569823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab82ef8f9b5d4ae58e5ad931fc224cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6756745269071556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64877d3348f4c1bb0296299777a573d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.6796753406524658 train acc 0.8125\n",
      "epoch 3 batch id 201 loss 0.7971736788749695 train acc 0.7209266169154229\n",
      "epoch 3 batch id 401 loss 0.7923454642295837 train acc 0.7227244389027432\n",
      "epoch 3 batch id 601 loss 0.4854649603366852 train acc 0.7250935940099834\n",
      "epoch 3 batch id 801 loss 0.7122911810874939 train acc 0.728854556803995\n",
      "epoch 3 batch id 1001 loss 0.8103989362716675 train acc 0.7299887612387612\n",
      "epoch 3 batch id 1201 loss 0.6338964700698853 train acc 0.7332431307243963\n",
      "epoch 3 batch id 1401 loss 0.744411289691925 train acc 0.7365497858672377\n",
      "epoch 3 batch id 1601 loss 0.7817335724830627 train acc 0.7397134603372892\n",
      "epoch 3 batch id 1801 loss 0.7209338545799255 train acc 0.7418621599111604\n",
      "epoch 3 batch id 2001 loss 0.8373080492019653 train acc 0.743815592203898\n",
      "epoch 3 batch id 2201 loss 1.1211225986480713 train acc 0.7460813266696956\n",
      "epoch 3 batch id 2401 loss 0.5144734978675842 train acc 0.7480086422324032\n",
      "epoch 3 batch id 2601 loss 0.7059633731842041 train acc 0.7504205113417917\n",
      "epoch 3 batch id 2801 loss 0.5483942627906799 train acc 0.7524098536237058\n",
      "epoch 3 batch id 3001 loss 0.4281752407550812 train acc 0.753998667110963\n",
      "epoch 3 batch id 3201 loss 0.5351054072380066 train acc 0.7544810215557638\n",
      "epoch 3 batch id 3401 loss 0.5080648064613342 train acc 0.7562849162011173\n",
      "epoch 3 train acc 0.7575073756673223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946213a9bf95424fbe973c0458e323e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6831774837374335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5984554ae2744d07be16ec4a993bc350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.47918447852134705 train acc 0.90625\n",
      "epoch 4 batch id 201 loss 0.5809556245803833 train acc 0.7831156716417911\n",
      "epoch 4 batch id 401 loss 0.6121208667755127 train acc 0.7826527431421446\n",
      "epoch 4 batch id 601 loss 0.25183266401290894 train acc 0.7872816139767055\n",
      "epoch 4 batch id 801 loss 0.5052704215049744 train acc 0.7911985018726592\n",
      "epoch 4 batch id 1001 loss 0.5975937247276306 train acc 0.7939872627372627\n",
      "epoch 4 batch id 1201 loss 0.2869487404823303 train acc 0.7960553705245629\n",
      "epoch 4 batch id 1401 loss 0.546409547328949 train acc 0.8002542826552462\n",
      "epoch 4 batch id 1601 loss 0.482979416847229 train acc 0.8038335415365396\n",
      "epoch 4 batch id 1801 loss 0.5907915830612183 train acc 0.8055420599666852\n",
      "epoch 4 batch id 2001 loss 0.8337492346763611 train acc 0.8071120689655172\n",
      "epoch 4 batch id 2201 loss 0.8659434914588928 train acc 0.8095184007269423\n",
      "epoch 4 batch id 2401 loss 0.3670334815979004 train acc 0.8114717825905873\n",
      "epoch 4 batch id 2601 loss 0.4871928095817566 train acc 0.8127643214148405\n",
      "epoch 4 batch id 2801 loss 0.4291216731071472 train acc 0.8145639950017851\n",
      "epoch 4 batch id 3001 loss 0.19607824087142944 train acc 0.8157072642452515\n",
      "epoch 4 batch id 3201 loss 0.2227160483598709 train acc 0.8161121524523587\n",
      "epoch 4 batch id 3401 loss 0.36737626791000366 train acc 0.8174709644222288\n",
      "epoch 4 train acc 0.8186551699915706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a416eaa7b0740a38ca010f5d0f59e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.676014562389119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bcc3b6723241cdb3976c6cb5e5844c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.25311607122421265 train acc 0.9375\n",
      "epoch 5 batch id 201 loss 0.36403998732566833 train acc 0.8426616915422885\n",
      "epoch 5 batch id 401 loss 0.5801221132278442 train acc 0.8400872817955112\n",
      "epoch 5 batch id 601 loss 0.13222357630729675 train acc 0.8403702163061564\n",
      "epoch 5 batch id 801 loss 0.37182262539863586 train acc 0.8433988764044944\n",
      "epoch 5 batch id 1001 loss 0.3651731610298157 train acc 0.8454982517482518\n",
      "epoch 5 batch id 1201 loss 0.3378058969974518 train acc 0.8463780183180682\n",
      "epoch 5 batch id 1401 loss 0.5292795896530151 train acc 0.8485010706638115\n",
      "epoch 5 batch id 1601 loss 0.41298744082450867 train acc 0.8509134915677702\n",
      "epoch 5 batch id 1801 loss 0.4454478323459625 train acc 0.8515234591893392\n",
      "epoch 5 batch id 2001 loss 0.6410144567489624 train acc 0.8524487756121939\n",
      "epoch 5 batch id 2201 loss 0.6913407444953918 train acc 0.853575079509314\n",
      "epoch 5 batch id 2401 loss 0.30798980593681335 train acc 0.8544616826322365\n",
      "epoch 5 batch id 2601 loss 0.3798934817314148 train acc 0.8549716455209535\n",
      "epoch 5 batch id 2801 loss 0.4731135666370392 train acc 0.8552749018207783\n",
      "epoch 5 batch id 3001 loss 0.19332604110240936 train acc 0.8555481506164612\n",
      "epoch 5 batch id 3201 loss 0.2813698649406433 train acc 0.854742658544205\n",
      "epoch 5 batch id 3401 loss 0.3221076726913452 train acc 0.855042634519259\n",
      "epoch 5 train acc 0.8551471621241922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bcb7dd0ec546b9a0a1454fad728bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.673521584861029\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model/kobert-v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 351.79 MB\n"
     ]
    }
   ],
   "source": [
    "# 모델 사이즈 확인(파라미터는 v1과 동일)\n",
    "import os\n",
    "\n",
    "model_path3 = 'model/kobert-v3.pt'\n",
    "size2 = os.path.getsize(model_path3) / (1024*1024) # mb 단위\n",
    "print(f\"Model size: {size2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
