{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 런타임 애러 방지\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>벌써 1년이 되어가네요시간이 참 빨라요!!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47700</th>\n",
       "      <td>선발진은 거의 두산 맞먹네.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101674</th>\n",
       "      <td>너무 맛있을 거 같아요, 할머님.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20011</th>\n",
       "      <td>미래가 비참하지 않으려면 지금 어느 정도 모아놓아야 할 텐데 걱정이야.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87099</th>\n",
       "      <td>고객층이 정~말 다양하시네요?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence  emotion\n",
       "180                     벌써 1년이 되어가네요시간이 참 빨라요!!        2\n",
       "47700                           선발진은 거의 두산 맞먹네.        2\n",
       "101674                       너무 맛있을 거 같아요, 할머님.        0\n",
       "20011   미래가 비참하지 않으려면 지금 어느 정도 모아놓아야 할 텐데 걱정이야.        3\n",
       "87099                          고객층이 정~말 다양하시네요?        5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v3b.csv', index_col=0) \n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de30864993bd4668bbeb679dcac9b73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.8405989408493042 train acc 0.15625\n",
      "epoch 1 batch id 201 loss 1.6611812114715576 train acc 0.24984452736318408\n",
      "epoch 1 batch id 401 loss 1.1772855520248413 train acc 0.33930798004987534\n",
      "epoch 1 batch id 601 loss 1.2624648809432983 train acc 0.40370216306156403\n",
      "epoch 1 batch id 801 loss 1.2549487352371216 train acc 0.44877496878901374\n",
      "epoch 1 batch id 1001 loss 1.0199912786483765 train acc 0.47986388611388614\n",
      "epoch 1 batch id 1201 loss 1.03671133518219 train acc 0.5033825978351374\n",
      "epoch 1 batch id 1401 loss 0.7906304001808167 train acc 0.5221270521056388\n",
      "epoch 1 batch id 1601 loss 0.8985372185707092 train acc 0.534548719550281\n",
      "epoch 1 batch id 1801 loss 0.6591906547546387 train acc 0.5436910049972238\n",
      "epoch 1 batch id 2001 loss 0.9145181775093079 train acc 0.5524112943528235\n",
      "epoch 1 batch id 2201 loss 0.8111550211906433 train acc 0.5603276919582009\n",
      "epoch 1 batch id 2401 loss 0.9330563545227051 train acc 0.5671204706372345\n",
      "epoch 1 batch id 2601 loss 0.9814738631248474 train acc 0.5723039215686274\n",
      "epoch 1 batch id 2801 loss 0.6290987133979797 train acc 0.5785768475544448\n",
      "epoch 1 batch id 3001 loss 0.955174446105957 train acc 0.5832847384205265\n",
      "epoch 1 batch id 3201 loss 1.0950835943222046 train acc 0.5878729303342706\n",
      "epoch 1 train acc 0.5915770477312905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f012382cc7b41168f13d11b1444a732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6622717903415782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41aab847a894b2f871b1d71ef9c9542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.498046875 train acc 0.375\n",
      "epoch 2 batch id 201 loss 1.09908127784729 train acc 0.6565609452736318\n",
      "epoch 2 batch id 401 loss 0.7351030707359314 train acc 0.6617051122194514\n",
      "epoch 2 batch id 601 loss 0.9609889984130859 train acc 0.6616056572379367\n",
      "epoch 2 batch id 801 loss 0.9599224925041199 train acc 0.6629993757802747\n",
      "epoch 2 batch id 1001 loss 0.8492099046707153 train acc 0.6629932567432567\n",
      "epoch 2 batch id 1201 loss 0.8906266689300537 train acc 0.6669702331390508\n",
      "epoch 2 batch id 1401 loss 0.723548412322998 train acc 0.6704809064953604\n",
      "epoch 2 batch id 1601 loss 0.7749738097190857 train acc 0.6726850405996252\n",
      "epoch 2 batch id 1801 loss 0.4804736375808716 train acc 0.6733064963908939\n",
      "epoch 2 batch id 2001 loss 0.854459285736084 train acc 0.6754591454272864\n",
      "epoch 2 batch id 2201 loss 0.650543749332428 train acc 0.6786261926397092\n",
      "epoch 2 batch id 2401 loss 0.7461828589439392 train acc 0.6806538942107455\n",
      "epoch 2 batch id 2601 loss 0.7593034505844116 train acc 0.682718185313341\n",
      "epoch 2 batch id 2801 loss 0.49797725677490234 train acc 0.68588227418779\n",
      "epoch 2 batch id 3001 loss 0.7710687518119812 train acc 0.688103965344885\n",
      "epoch 2 batch id 3201 loss 0.9173392057418823 train acc 0.6906630740393627\n",
      "epoch 2 train acc 0.6921129198585739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115b3efad5934eb6add16699e882a547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6741975853945819\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf2a1809df94414a0be2e21f58b5157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 1.2704143524169922 train acc 0.5\n",
      "epoch 3 batch id 201 loss 0.95688796043396 train acc 0.7083333333333334\n",
      "epoch 3 batch id 401 loss 0.4161251485347748 train acc 0.720464463840399\n",
      "epoch 3 batch id 601 loss 0.6803663372993469 train acc 0.7237936772046589\n",
      "epoch 3 batch id 801 loss 0.9167876839637756 train acc 0.7260065543071161\n",
      "epoch 3 batch id 1001 loss 0.6575143337249756 train acc 0.7262112887112887\n",
      "epoch 3 batch id 1201 loss 0.5818576216697693 train acc 0.7302508326394671\n",
      "epoch 3 batch id 1401 loss 0.5183041095733643 train acc 0.7334716274089935\n",
      "epoch 3 batch id 1601 loss 0.6896904110908508 train acc 0.7368246408494691\n",
      "epoch 3 batch id 1801 loss 0.34975388646125793 train acc 0.7378886729594669\n",
      "epoch 3 batch id 2001 loss 0.6799705028533936 train acc 0.7398332083958021\n",
      "epoch 3 batch id 2201 loss 0.4767628014087677 train acc 0.7427163789186734\n",
      "epoch 3 batch id 2401 loss 0.5166285037994385 train acc 0.7449109745939192\n",
      "epoch 3 batch id 2601 loss 0.6827917695045471 train acc 0.7469242599000384\n",
      "epoch 3 batch id 2801 loss 0.38201168179512024 train acc 0.7501450374866119\n",
      "epoch 3 batch id 3001 loss 0.8744123578071594 train acc 0.7524679273575475\n",
      "epoch 3 batch id 3201 loss 0.6471662521362305 train acc 0.7547153233364574\n",
      "epoch 3 train acc 0.7563439157336476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6935902351411b9f74cc83fc9d864c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6826855123674912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476e85f9ed044f7d83cb7e65a462ecf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.8032402992248535 train acc 0.6875\n",
      "epoch 4 batch id 201 loss 1.0090919733047485 train acc 0.7756529850746269\n",
      "epoch 4 batch id 401 loss 0.3548327088356018 train acc 0.7813279301745636\n",
      "epoch 4 batch id 601 loss 0.5835792422294617 train acc 0.7855137271214643\n",
      "epoch 4 batch id 801 loss 0.8195212483406067 train acc 0.7874141697877652\n",
      "epoch 4 batch id 1001 loss 0.4579055905342102 train acc 0.7895854145854145\n",
      "epoch 4 batch id 1201 loss 0.4711407423019409 train acc 0.7932972522897586\n",
      "epoch 4 batch id 1401 loss 0.42371252179145813 train acc 0.7968415417558886\n",
      "epoch 4 batch id 1601 loss 0.3355950713157654 train acc 0.7999102123672704\n",
      "epoch 4 batch id 1801 loss 0.22865699231624603 train acc 0.8010480288728484\n",
      "epoch 4 batch id 2001 loss 0.5283963680267334 train acc 0.8030515992003998\n",
      "epoch 4 batch id 2201 loss 0.3444550931453705 train acc 0.8052873693775556\n",
      "epoch 4 batch id 2401 loss 0.3987598419189453 train acc 0.8073068513119533\n",
      "epoch 4 batch id 2601 loss 0.4600212275981903 train acc 0.8087634563629373\n",
      "epoch 4 batch id 2801 loss 0.18646498024463654 train acc 0.8109492145662264\n",
      "epoch 4 batch id 3001 loss 0.9352254867553711 train acc 0.8126249583472176\n",
      "epoch 4 batch id 3201 loss 0.4158526360988617 train acc 0.8144525148391127\n",
      "epoch 4 train acc 0.8160356511490866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6965e380fb4b55832693c33bd3ee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.682243816254417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86165664f9374071b190a95a54ef8304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.4588736891746521 train acc 0.78125\n",
      "epoch 5 batch id 201 loss 0.7597526907920837 train acc 0.8337997512437811\n",
      "epoch 5 batch id 401 loss 0.16679087281227112 train acc 0.8405548628428927\n",
      "epoch 5 batch id 601 loss 0.5324738025665283 train acc 0.8410981697171381\n",
      "epoch 5 batch id 801 loss 0.6838040351867676 train acc 0.8412921348314607\n",
      "epoch 5 batch id 1001 loss 0.43277183175086975 train acc 0.8419393106893107\n",
      "epoch 5 batch id 1201 loss 0.44571810960769653 train acc 0.8434637801831807\n",
      "epoch 5 batch id 1401 loss 0.22327153384685516 train acc 0.8455121341898644\n",
      "epoch 5 batch id 1601 loss 0.3069186210632324 train acc 0.8475757339163024\n",
      "epoch 5 batch id 1801 loss 0.13263973593711853 train acc 0.8483654913936702\n",
      "epoch 5 batch id 2001 loss 0.38930419087409973 train acc 0.8490129935032483\n",
      "epoch 5 batch id 2201 loss 0.4218185842037201 train acc 0.8498125851885506\n",
      "epoch 5 batch id 2401 loss 0.2279215157032013 train acc 0.8509345064556435\n",
      "epoch 5 batch id 2601 loss 0.5570409297943115 train acc 0.8522323144944253\n",
      "epoch 5 batch id 2801 loss 0.2345593273639679 train acc 0.8535121385219564\n",
      "epoch 5 batch id 3001 loss 0.6673808097839355 train acc 0.8544443518827057\n",
      "epoch 5 batch id 3201 loss 0.32924574613571167 train acc 0.8552307872539832\n",
      "epoch 5 train acc 0.8555631261048909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dead3c4e74dc48af894df4652a75e6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6777826855123675\n"
     ]
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy = [], []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_accuarcy.append(train_acc / (batch_id+1))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    test_accuarcy.append(test_acc / (batch_id+1))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5915770477312905,\n",
       "  0.6921129198585739,\n",
       "  0.7563439157336476,\n",
       "  0.8160356511490866,\n",
       "  0.8555631261048909],\n",
       " [0.6622717903415782,\n",
       "  0.6741975853945819,\n",
       "  0.6826855123674912,\n",
       "  0.682243816254417,\n",
       "  0.6777826855123675])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 런타임 애러 방지\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99498</th>\n",
       "      <td>담화할 시간은 있고 검찰조사 받을 시간은 없고?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24242</th>\n",
       "      <td>치이... 음악제 나간다는 곡 다 만들었어? 다 만들면 들어봐주기로 했잖아, 내가.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65250</th>\n",
       "      <td>나는 부모님이 사소한 일에도 이래라저래라해서 너무 신경 쓰여.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69867</th>\n",
       "      <td>학교가 적성에 안 맞는데 진지하게 자퇴를 해야 할지 그냥 다녀야 할지 혼란스러워.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>촛불집회로 청와대 검찰까지 점령하자</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  emotion\n",
       "99498                       담화할 시간은 있고 검찰조사 받을 시간은 없고?        4\n",
       "24242  치이... 음악제 나간다는 곡 다 만들었어? 다 만들면 들어봐주기로 했잖아, 내가.         5\n",
       "65250               나는 부모님이 사소한 일에도 이래라저래라해서 너무 신경 쓰여.        3\n",
       "69867    학교가 적성에 안 맞는데 진지하게 자퇴를 해야 할지 그냥 다녀야 할지 혼란스러워.        2\n",
       "19613                              촛불집회로 청와대 검찰까지 점령하자        5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v4.csv', index_col=0) \n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3990ba54d5e047ad821b77e88da5d196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.811486840248108 train acc 0.1875\n",
      "epoch 1 batch id 201 loss 1.7802951335906982 train acc 0.28467039800995025\n",
      "epoch 1 batch id 401 loss 1.425218105316162 train acc 0.3637001246882793\n",
      "epoch 1 batch id 601 loss 1.037771463394165 train acc 0.420757071547421\n",
      "epoch 1 batch id 801 loss 1.1631488800048828 train acc 0.460440074906367\n",
      "epoch 1 batch id 1001 loss 1.0462727546691895 train acc 0.4906968031968032\n",
      "epoch 1 batch id 1201 loss 1.0188231468200684 train acc 0.5118390924229809\n",
      "epoch 1 batch id 1401 loss 1.163686990737915 train acc 0.5269450392576731\n",
      "epoch 1 batch id 1601 loss 0.7133719325065613 train acc 0.5397993441599\n",
      "epoch 1 batch id 1801 loss 1.0510989427566528 train acc 0.5492434758467518\n",
      "epoch 1 batch id 2001 loss 0.8356772661209106 train acc 0.5585176161919041\n",
      "epoch 1 batch id 2201 loss 1.1321587562561035 train acc 0.5656661744661517\n",
      "epoch 1 batch id 2401 loss 1.051943302154541 train acc 0.5718190337359433\n",
      "epoch 1 batch id 2601 loss 0.9795247912406921 train acc 0.5776624375240292\n",
      "epoch 1 batch id 2801 loss 0.954567551612854 train acc 0.5826490539093181\n",
      "epoch 1 batch id 3001 loss 1.5028783082962036 train acc 0.587585388203932\n",
      "epoch 1 batch id 3201 loss 1.0963430404663086 train acc 0.5919341611996252\n",
      "epoch 1 train acc 0.5950254706447912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfe079c2e8c4644bbe3c83770e7240a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6638013047023649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccad0c8ac6d42b08feba90e1e88ff6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.7878365516662598 train acc 0.71875\n",
      "epoch 2 batch id 201 loss 1.2827378511428833 train acc 0.6573383084577115\n",
      "epoch 2 batch id 401 loss 0.9302335977554321 train acc 0.6629519950124688\n",
      "epoch 2 batch id 601 loss 0.7089939117431641 train acc 0.665765391014975\n",
      "epoch 2 batch id 801 loss 1.1416207551956177 train acc 0.6657693508114857\n",
      "epoch 2 batch id 1001 loss 0.9826791882514954 train acc 0.6679258241758241\n",
      "epoch 2 batch id 1201 loss 0.9294556379318237 train acc 0.670092631140716\n",
      "epoch 2 batch id 1401 loss 0.9485880136489868 train acc 0.6709939329050678\n",
      "epoch 2 batch id 1601 loss 0.5654458403587341 train acc 0.6746564647095565\n",
      "epoch 2 batch id 1801 loss 0.6567437648773193 train acc 0.6770544142143253\n",
      "epoch 2 batch id 2001 loss 0.8190523982048035 train acc 0.6793165917041479\n",
      "epoch 2 batch id 2201 loss 1.0485857725143433 train acc 0.6820195365742844\n",
      "epoch 2 batch id 2401 loss 0.7834093570709229 train acc 0.6842461474385673\n",
      "epoch 2 batch id 2601 loss 0.8431361317634583 train acc 0.6860942906574394\n",
      "epoch 2 batch id 2801 loss 0.6837155818939209 train acc 0.6873438057836487\n",
      "epoch 2 batch id 3001 loss 0.9705906510353088 train acc 0.6900199933355549\n",
      "epoch 2 batch id 3201 loss 0.9415381550788879 train acc 0.6920298344267416\n",
      "epoch 2 train acc 0.692995863443228\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b3a31df21242889b1dc89d2fd8ec31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6740509196339585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c66484335f43609700191b7f79b43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.6175364255905151 train acc 0.75\n",
      "epoch 3 batch id 201 loss 1.296385407447815 train acc 0.7255907960199005\n",
      "epoch 3 batch id 401 loss 0.6983014941215515 train acc 0.7272443890274314\n",
      "epoch 3 batch id 601 loss 0.4794703722000122 train acc 0.7295133111480865\n",
      "epoch 3 batch id 801 loss 0.8395825624465942 train acc 0.7286204744069913\n",
      "epoch 3 batch id 1001 loss 0.9233671426773071 train acc 0.7303321678321678\n",
      "epoch 3 batch id 1201 loss 0.7946392297744751 train acc 0.7330609908409659\n",
      "epoch 3 batch id 1401 loss 0.8191417455673218 train acc 0.7352560670949322\n",
      "epoch 3 batch id 1601 loss 0.4188605546951294 train acc 0.7392840412242349\n",
      "epoch 3 batch id 1801 loss 0.4464825391769409 train acc 0.7413069128262076\n",
      "epoch 3 batch id 2001 loss 0.5248220562934875 train acc 0.7442060219890055\n",
      "epoch 3 batch id 2201 loss 0.9251001477241516 train acc 0.7476573148568832\n",
      "epoch 3 batch id 2401 loss 0.5123996734619141 train acc 0.7498958767180341\n",
      "epoch 3 batch id 2601 loss 0.8486307859420776 train acc 0.7516580161476355\n",
      "epoch 3 batch id 2801 loss 0.43700161576271057 train acc 0.7534028025705105\n",
      "epoch 3 batch id 3001 loss 0.5866838097572327 train acc 0.7556127124291903\n",
      "epoch 3 batch id 3201 loss 0.7635409832000732 train acc 0.757321930646673\n",
      "epoch 3 train acc 0.7583443499364203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de430589e9a4435382a67bebc80b2aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6789662045845792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb249477c71c48d686ce3902fc75a7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.4814194142818451 train acc 0.84375\n",
      "epoch 4 batch id 201 loss 1.1751004457473755 train acc 0.7907338308457711\n",
      "epoch 4 batch id 401 loss 0.5371299386024475 train acc 0.7967581047381546\n",
      "epoch 4 batch id 601 loss 0.3577406406402588 train acc 0.7976809484193012\n",
      "epoch 4 batch id 801 loss 0.5434825420379639 train acc 0.797869850187266\n",
      "epoch 4 batch id 1001 loss 0.7746517658233643 train acc 0.8003246753246753\n",
      "epoch 4 batch id 1201 loss 0.822655200958252 train acc 0.8015716069941715\n",
      "epoch 4 batch id 1401 loss 0.7087211608886719 train acc 0.8030424696645253\n",
      "epoch 4 batch id 1601 loss 0.2680892050266266 train acc 0.8054145846346034\n",
      "epoch 4 batch id 1801 loss 0.2751755714416504 train acc 0.8072251526929484\n",
      "epoch 4 batch id 2001 loss 0.3100835978984833 train acc 0.8097513743128436\n",
      "epoch 4 batch id 2201 loss 0.696457028388977 train acc 0.812315424806906\n",
      "epoch 4 batch id 2401 loss 0.5053235292434692 train acc 0.8139186797167847\n",
      "epoch 4 batch id 2601 loss 0.45906496047973633 train acc 0.8149870242214533\n",
      "epoch 4 batch id 2801 loss 0.3926013112068176 train acc 0.8161147804355587\n",
      "epoch 4 batch id 3001 loss 0.2555248737335205 train acc 0.8179252749083639\n",
      "epoch 4 batch id 3201 loss 0.8437592387199402 train acc 0.8192557013433303\n",
      "epoch 4 train acc 0.8199299072666936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c32ce206454556b3f1d2b7618a4510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6755430597082541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3269c24bc54f8696eb2c2b2f032140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.27501434087753296 train acc 0.96875\n",
      "epoch 5 batch id 201 loss 1.0902817249298096 train acc 0.8454601990049752\n",
      "epoch 5 batch id 401 loss 0.3095819056034088 train acc 0.8491271820448878\n",
      "epoch 5 batch id 601 loss 0.18720588088035583 train acc 0.8496776206322796\n",
      "epoch 5 batch id 801 loss 0.4377713203430176 train acc 0.8485096754057428\n",
      "epoch 5 batch id 1001 loss 0.7474216818809509 train acc 0.8504308191808192\n",
      "epoch 5 batch id 1201 loss 0.7089880108833313 train acc 0.8507493755203996\n",
      "epoch 5 batch id 1401 loss 0.4744078516960144 train acc 0.8512892576730906\n",
      "epoch 5 batch id 1601 loss 0.16201433539390564 train acc 0.8528068394753279\n",
      "epoch 5 batch id 1801 loss 0.15278543531894684 train acc 0.8529289283731261\n",
      "epoch 5 batch id 2001 loss 0.2478848546743393 train acc 0.8542603698150925\n",
      "epoch 5 batch id 2201 loss 0.5300540924072266 train acc 0.856230122671513\n",
      "epoch 5 batch id 2401 loss 0.38902610540390015 train acc 0.8565962099125365\n",
      "epoch 5 batch id 2601 loss 0.4132373332977295 train acc 0.8565816032295271\n",
      "epoch 5 batch id 2801 loss 0.215056911110878 train acc 0.8571827026062121\n",
      "epoch 5 batch id 3001 loss 0.2119872272014618 train acc 0.8577869876707764\n",
      "epoch 5 batch id 3201 loss 0.6080772876739502 train acc 0.8582571852546079\n",
      "epoch 5 train acc 0.8580026090934466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fe166dfdfc41dbb5ea17748e785ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6754326356799856\n"
     ]
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy = [], []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_accuarcy.append(train_acc / (batch_id+1))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    test_accuarcy.append(test_acc / (batch_id+1))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuarcy, test_accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'model/kobert-v7.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
