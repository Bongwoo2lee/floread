{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 런타임 애러 방지\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>빨리 들어가서 일이나 봐아...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66035</th>\n",
       "      <td>김 여사가 나눠 준 음식에 땅콩이 있었어. 모르고 먹었는데 고생했어.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15418</th>\n",
       "      <td>딱 이거다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11171</th>\n",
       "      <td>내가 돈이 없어서 취업을 해야 하는데 나이도 많고 해서 나를 원하는 직장이 없을까 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61891</th>\n",
       "      <td>은퇴할 때는 점점 다가오는데 자식들이 커서 나한테 도움을 주지 않을까 봐 무서워.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  emotion\n",
       "37820                                  빨리 들어가서 일이나 봐아...        5\n",
       "66035             김 여사가 나눠 준 음식에 땅콩이 있었어. 모르고 먹었는데 고생했어.        1\n",
       "15418                                             딱 이거다.        2\n",
       "11171  내가 돈이 없어서 취업을 해야 하는데 나이도 많고 해서 나를 원하는 직장이 없을까 ...        1\n",
       "61891      은퇴할 때는 점점 다가오는데 자식들이 커서 나한테 도움을 주지 않을까 봐 무서워.        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v3.csv', index_col=0) \n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3387db74d29543b5a031be35481dadc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.765323519706726 train acc 0.25\n",
      "epoch 1 batch id 201 loss 1.5442984104156494 train acc 0.2484452736318408\n",
      "epoch 1 batch id 401 loss 1.669565200805664 train acc 0.3455423940149626\n",
      "epoch 1 batch id 601 loss 1.216027855873108 train acc 0.4097337770382696\n",
      "epoch 1 batch id 801 loss 1.1335647106170654 train acc 0.4554463171036205\n",
      "epoch 1 batch id 1001 loss 1.2969141006469727 train acc 0.4875124875124875\n",
      "epoch 1 batch id 1201 loss 1.1551012992858887 train acc 0.510876353039134\n",
      "epoch 1 batch id 1401 loss 1.1454170942306519 train acc 0.5270342612419701\n",
      "epoch 1 batch id 1601 loss 1.197957158088684 train acc 0.5380231105559026\n",
      "epoch 1 batch id 1801 loss 0.8738500475883484 train acc 0.5486361743475847\n",
      "epoch 1 batch id 2001 loss 1.2041116952896118 train acc 0.5578616941529235\n",
      "epoch 1 batch id 2201 loss 0.9557017087936401 train acc 0.5655383916401635\n",
      "epoch 1 batch id 2401 loss 0.8528918027877808 train acc 0.5720923573511038\n",
      "epoch 1 batch id 2601 loss 0.7924847602844238 train acc 0.5772899846212995\n",
      "epoch 1 batch id 2801 loss 0.7214281558990479 train acc 0.582470546233488\n",
      "epoch 1 batch id 3001 loss 0.9692776799201965 train acc 0.5868772909030323\n",
      "epoch 1 batch id 3201 loss 0.8715335130691528 train acc 0.5915436582318025\n",
      "epoch 1 train acc 0.5954037800687285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1892ded41c844389d6a99d8bdf339e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.674612395657295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81767fe7ceb420695e29b645a377142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.9162861704826355 train acc 0.65625\n",
      "epoch 2 batch id 201 loss 0.8593024015426636 train acc 0.6548507462686567\n",
      "epoch 2 batch id 401 loss 1.124955654144287 train acc 0.6588216957605985\n",
      "epoch 2 batch id 601 loss 0.7193484306335449 train acc 0.6603057404326124\n",
      "epoch 2 batch id 801 loss 1.0079941749572754 train acc 0.6621800873907615\n",
      "epoch 2 batch id 1001 loss 1.2259600162506104 train acc 0.6663648851148851\n",
      "epoch 2 batch id 1201 loss 0.9761208295822144 train acc 0.6692599916736053\n",
      "epoch 2 batch id 1401 loss 1.0396537780761719 train acc 0.6706816559600286\n",
      "epoch 2 batch id 1601 loss 1.1270655393600464 train acc 0.6727631168019987\n",
      "epoch 2 batch id 1801 loss 0.5492494106292725 train acc 0.6761347862298723\n",
      "epoch 2 batch id 2001 loss 1.1417326927185059 train acc 0.6786762868565717\n",
      "epoch 2 batch id 2201 loss 0.7592246532440186 train acc 0.6810824625170377\n",
      "epoch 2 batch id 2401 loss 0.5758426189422607 train acc 0.6832960224906289\n",
      "epoch 2 batch id 2601 loss 0.522323727607727 train acc 0.6849168589004229\n",
      "epoch 2 batch id 2801 loss 0.6511735320091248 train acc 0.6869310067832917\n",
      "epoch 2 batch id 3001 loss 0.8504756689071655 train acc 0.6883955348217261\n",
      "epoch 2 batch id 3201 loss 0.7163596153259277 train acc 0.6906435488909716\n",
      "epoch 2 train acc 0.6923140648011782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7c3142dcbe4be1abfb64920cf736c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6846609822297333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6413ef589df94b22ad852582b3b38104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.7986209392547607 train acc 0.75\n",
      "epoch 3 batch id 201 loss 0.7797208428382874 train acc 0.7184390547263682\n",
      "epoch 3 batch id 401 loss 0.8271353244781494 train acc 0.7216334164588528\n",
      "epoch 3 batch id 601 loss 0.4576512575149536 train acc 0.7220257903494176\n",
      "epoch 3 batch id 801 loss 0.6782790422439575 train acc 0.7240168539325843\n",
      "epoch 3 batch id 1001 loss 1.2290399074554443 train acc 0.7277097902097902\n",
      "epoch 3 batch id 1201 loss 0.8539198040962219 train acc 0.7309533721898418\n",
      "epoch 3 batch id 1401 loss 0.8623064756393433 train acc 0.7329586009992862\n",
      "epoch 3 batch id 1601 loss 0.8802357316017151 train acc 0.7352631168019987\n",
      "epoch 3 batch id 1801 loss 0.3300066590309143 train acc 0.7386868406440866\n",
      "epoch 3 batch id 2001 loss 0.8269587159156799 train acc 0.7418790604697652\n",
      "epoch 3 batch id 2201 loss 0.4913092255592346 train acc 0.7446473194002726\n",
      "epoch 3 batch id 2401 loss 0.5226374864578247 train acc 0.7472797792586422\n",
      "epoch 3 batch id 2601 loss 0.3548867702484131 train acc 0.7491950211457132\n",
      "epoch 3 batch id 2801 loss 0.6121973991394043 train acc 0.7513499642984648\n",
      "epoch 3 batch id 3001 loss 0.8289956450462341 train acc 0.7533426357880706\n",
      "epoch 3 batch id 3201 loss 0.6084923148155212 train acc 0.755730631052796\n",
      "epoch 3 train acc 0.7571244477172312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7568bd3114443eba4066ed82a4ecf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6906606877656578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4d08e9522540b690dcc43c1c29f631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.5337974429130554 train acc 0.8125\n",
      "epoch 4 batch id 201 loss 0.676946222782135 train acc 0.7782960199004975\n",
      "epoch 4 batch id 401 loss 0.6255374550819397 train acc 0.782964463840399\n",
      "epoch 4 batch id 601 loss 0.3529127240180969 train acc 0.7839538269550749\n",
      "epoch 4 batch id 801 loss 0.5618844032287598 train acc 0.7865948813982522\n",
      "epoch 4 batch id 1001 loss 0.8978216648101807 train acc 0.7913336663336663\n",
      "epoch 4 batch id 1201 loss 0.5107479095458984 train acc 0.7941819317235637\n",
      "epoch 4 batch id 1401 loss 0.6392253041267395 train acc 0.7959493219129193\n",
      "epoch 4 batch id 1601 loss 0.7917352914810181 train acc 0.7980168644597126\n",
      "epoch 4 batch id 1801 loss 0.2538401186466217 train acc 0.8008051082731815\n",
      "epoch 4 batch id 2001 loss 0.5556666254997253 train acc 0.803691904047976\n",
      "epoch 4 batch id 2201 loss 0.424376904964447 train acc 0.8065509995456611\n",
      "epoch 4 batch id 2401 loss 0.3112984895706177 train acc 0.8088817159516868\n",
      "epoch 4 batch id 2601 loss 0.20947566628456116 train acc 0.8110101883890811\n",
      "epoch 4 batch id 2801 loss 0.3598766326904297 train acc 0.81267850767583\n",
      "epoch 4 batch id 3001 loss 0.6112903356552124 train acc 0.8147075974675109\n",
      "epoch 4 batch id 3201 loss 0.3092249035835266 train acc 0.8164343174008123\n",
      "epoch 4 train acc 0.8173539518900343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1de08e78aa34d009eedd34032ef3066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6839248220412762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0456227f341e4471ab7050d6d50f4b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.5969951152801514 train acc 0.78125\n",
      "epoch 5 batch id 201 loss 0.5590223073959351 train acc 0.8310012437810945\n",
      "epoch 5 batch id 401 loss 0.6003820300102234 train acc 0.8341645885286783\n",
      "epoch 5 batch id 601 loss 0.5485747456550598 train acc 0.8362624792013311\n",
      "epoch 5 batch id 801 loss 0.262102335691452 train acc 0.8376248439450686\n",
      "epoch 5 batch id 1001 loss 0.643107533454895 train acc 0.8406905594405595\n",
      "epoch 5 batch id 1201 loss 0.25834202766418457 train acc 0.84375\n",
      "epoch 5 batch id 1401 loss 0.5387965440750122 train acc 0.8443522483940042\n",
      "epoch 5 batch id 1601 loss 0.5043561458587646 train acc 0.8455457526545909\n",
      "epoch 5 batch id 1801 loss 0.1885564774274826 train acc 0.8471682398667407\n",
      "epoch 5 batch id 2001 loss 0.5866328477859497 train acc 0.8491223138430785\n",
      "epoch 5 batch id 2201 loss 0.3297790586948395 train acc 0.8507212630622444\n",
      "epoch 5 batch id 2401 loss 0.21570153534412384 train acc 0.8518455851728447\n",
      "epoch 5 batch id 2601 loss 0.2142801433801651 train acc 0.8528570742022299\n",
      "epoch 5 batch id 2801 loss 0.36805492639541626 train acc 0.8538468404141378\n",
      "epoch 5 batch id 3001 loss 0.40455371141433716 train acc 0.8544860046651116\n",
      "epoch 5 batch id 3201 loss 0.3097350597381592 train acc 0.8551038737894407\n",
      "epoch 5 train acc 0.8551515709376535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b132b8cb64047efbf83221578748a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6863093409125826\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model/kobert-v5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 351.79 MB\n"
     ]
    }
   ],
   "source": [
    "# 모델 사이즈 확인(파라미터 동일)\n",
    "import os\n",
    "\n",
    "model_path = 'model/kobert-v5.pt'\n",
    "size = os.path.getsize(model_path) / (1024*1024) # mb 단위\n",
    "print(f\"Model size: {size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
