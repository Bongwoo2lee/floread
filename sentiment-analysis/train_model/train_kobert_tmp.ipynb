{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147525, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v5-bt.csv', index_col=0)\n",
    "\n",
    "train_set = train_set.dropna()\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "# 데이터 프레임 셔플\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 7  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(float(i[label_idx])) for i in dataset]         # self.labels = [np.int32(i[label_idx]) for i in dataset] 이꺼 변경\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류\n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3af150d7bac45ac8cda7b3206d80eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.7849496603012085 train acc 0.28125\n",
      "epoch 1 batch id 201 loss 1.7854769229888916 train acc 0.22046019900497513\n",
      "epoch 1 batch id 401 loss 1.5321274995803833 train acc 0.26317019950124687\n",
      "epoch 1 batch id 601 loss 1.5046125650405884 train acc 0.30870424292845255\n",
      "epoch 1 batch id 801 loss 1.187895655632019 train acc 0.3533863920099875\n",
      "epoch 1 batch id 1001 loss 1.410932183265686 train acc 0.38920454545454547\n",
      "epoch 1 batch id 1201 loss 1.1485865116119385 train acc 0.41980641132389673\n",
      "epoch 1 batch id 1401 loss 1.1155807971954346 train acc 0.4452177016416845\n",
      "epoch 1 batch id 1601 loss 1.1803524494171143 train acc 0.46720799500312304\n",
      "epoch 1 batch id 1801 loss 1.0862274169921875 train acc 0.48481746252082175\n",
      "epoch 1 batch id 2001 loss 0.9968980550765991 train acc 0.5001717891054472\n",
      "epoch 1 batch id 2201 loss 1.055002212524414 train acc 0.5129060654248069\n",
      "epoch 1 batch id 2401 loss 0.8070299029350281 train acc 0.524039462723865\n",
      "epoch 1 batch id 2601 loss 0.9205293655395508 train acc 0.5333405420991926\n",
      "epoch 1 batch id 2801 loss 0.9258934855461121 train acc 0.5415253480899679\n",
      "epoch 1 batch id 3001 loss 0.700446605682373 train acc 0.5489524325224925\n",
      "epoch 1 batch id 3201 loss 0.970953106880188 train acc 0.5557052483598875\n",
      "epoch 1 batch id 3401 loss 0.9439408183097839 train acc 0.5612319905910026\n",
      "epoch 1 batch id 3601 loss 0.7339929938316345 train acc 0.5667522910302694\n",
      "epoch 1 train acc 0.5687432230956899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25eae419a1b04db2baa969156c1d5172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6718242145178764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88826f29b1d64779ba97d4e298c4a914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.6825827360153198 train acc 0.8125\n",
      "epoch 2 batch id 201 loss 0.8812358379364014 train acc 0.6682213930348259\n",
      "epoch 2 batch id 401 loss 0.9113104939460754 train acc 0.6691084788029925\n",
      "epoch 2 batch id 601 loss 0.7585620284080505 train acc 0.6692491680532446\n",
      "epoch 2 batch id 801 loss 0.6849930882453918 train acc 0.6703339575530587\n",
      "epoch 2 batch id 1001 loss 0.9784746170043945 train acc 0.6697677322677322\n",
      "epoch 2 batch id 1201 loss 0.9262698888778687 train acc 0.670951290591174\n",
      "epoch 2 batch id 1401 loss 0.8358215689659119 train acc 0.6724884011420414\n",
      "epoch 2 batch id 1601 loss 0.8659276366233826 train acc 0.6748711742660837\n",
      "epoch 2 batch id 1801 loss 1.0283232927322388 train acc 0.6760827318156579\n",
      "epoch 2 batch id 2001 loss 0.8918014764785767 train acc 0.6778641929035483\n",
      "epoch 2 batch id 2201 loss 0.9112681746482849 train acc 0.679719445706497\n",
      "epoch 2 batch id 2401 loss 0.6372177004814148 train acc 0.6822678050812162\n",
      "epoch 2 batch id 2601 loss 0.9279241561889648 train acc 0.6844843329488658\n",
      "epoch 2 batch id 2801 loss 0.8314332962036133 train acc 0.6866520885398072\n",
      "epoch 2 batch id 3001 loss 0.45964425802230835 train acc 0.688728757080973\n",
      "epoch 2 batch id 3201 loss 0.7062054872512817 train acc 0.6902628084973446\n",
      "epoch 2 batch id 3401 loss 0.7889003753662109 train acc 0.6914877977065569\n",
      "epoch 2 batch id 3601 loss 0.6629332900047302 train acc 0.6929411968897529\n",
      "epoch 2 train acc 0.6934213201409596\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd9fe15b4a44071a77025f9cb5ac9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6864504333694474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fd5778d10444de9a7b815edfb380c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.5779872536659241 train acc 0.8125\n",
      "epoch 3 batch id 201 loss 0.6856139898300171 train acc 0.7133084577114428\n",
      "epoch 3 batch id 401 loss 0.6881759166717529 train acc 0.7215554862842892\n",
      "epoch 3 batch id 601 loss 0.655266284942627 train acc 0.7193219633943427\n",
      "epoch 3 batch id 801 loss 0.4680611193180084 train acc 0.7208957553058677\n",
      "epoch 3 batch id 1001 loss 0.759028971195221 train acc 0.7208104395604396\n",
      "epoch 3 batch id 1201 loss 0.757350504398346 train acc 0.7210657785179018\n",
      "epoch 3 batch id 1401 loss 0.8638201355934143 train acc 0.7228988222698073\n",
      "epoch 3 batch id 1601 loss 0.7672981023788452 train acc 0.7245276389756402\n",
      "epoch 3 batch id 1801 loss 1.0260647535324097 train acc 0.7257599944475291\n",
      "epoch 3 batch id 2001 loss 0.5897591710090637 train acc 0.7279641429285357\n",
      "epoch 3 batch id 2201 loss 0.7731530070304871 train acc 0.7298955020445252\n",
      "epoch 3 batch id 2401 loss 0.48406434059143066 train acc 0.7319996876301541\n",
      "epoch 3 batch id 2601 loss 0.7107743620872498 train acc 0.7340686274509803\n",
      "epoch 3 batch id 2801 loss 0.6122077107429504 train acc 0.7358086397715101\n",
      "epoch 3 batch id 3001 loss 0.35077905654907227 train acc 0.737847800733089\n",
      "epoch 3 batch id 3201 loss 0.6615203022956848 train acc 0.7394271321462043\n",
      "epoch 3 batch id 3401 loss 0.6539997458457947 train acc 0.740609379594237\n",
      "epoch 3 batch id 3601 loss 0.46086788177490234 train acc 0.7412697861705082\n",
      "epoch 3 train acc 0.741766061263215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc5438171644be3962a941383bb1425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6829293066088841\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602cae22ff7a4b008e0569a6ba3320fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.3139314651489258 train acc 0.9375\n",
      "epoch 4 batch id 201 loss 0.6290171146392822 train acc 0.7671019900497512\n",
      "epoch 4 batch id 401 loss 0.812953531742096 train acc 0.7699501246882793\n",
      "epoch 4 batch id 601 loss 0.552045464515686 train acc 0.7666909317803661\n",
      "epoch 4 batch id 801 loss 0.37679141759872437 train acc 0.7672050561797753\n",
      "epoch 4 batch id 1001 loss 0.5712737441062927 train acc 0.7660152347652348\n",
      "epoch 4 batch id 1201 loss 0.6766979098320007 train acc 0.7653517901748543\n",
      "epoch 4 batch id 1401 loss 0.6807597875595093 train acc 0.767063704496788\n",
      "epoch 4 batch id 1601 loss 0.6692736744880676 train acc 0.7688749219237976\n",
      "epoch 4 batch id 1801 loss 0.8165844678878784 train acc 0.7710299833425874\n",
      "epoch 4 batch id 2001 loss 0.5039604306221008 train acc 0.7728948025987007\n",
      "epoch 4 batch id 2201 loss 0.6489577889442444 train acc 0.7738811903680145\n",
      "epoch 4 batch id 2401 loss 0.31412971019744873 train acc 0.7761219283631821\n",
      "epoch 4 batch id 2601 loss 0.7411506772041321 train acc 0.7779700115340253\n",
      "epoch 4 batch id 2801 loss 0.5527486205101013 train acc 0.7799111924312745\n",
      "epoch 4 batch id 3001 loss 0.2368895709514618 train acc 0.7819580973008997\n",
      "epoch 4 batch id 3201 loss 0.7093803286552429 train acc 0.7834758669165885\n",
      "epoch 4 batch id 3401 loss 0.5134384036064148 train acc 0.784328138782711\n",
      "epoch 4 batch id 3601 loss 0.32567185163497925 train acc 0.7853026936961955\n",
      "epoch 4 train acc 0.7856126321496341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf37dd21e684f0e93e8669a846582c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6873645720476707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc6aec9af354e62a16f07cb014d72f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.21377032995224 train acc 0.96875\n",
      "epoch 5 batch id 201 loss 0.5141083598136902 train acc 0.8173196517412935\n",
      "epoch 5 batch id 401 loss 0.6125074028968811 train acc 0.8153054862842892\n",
      "epoch 5 batch id 601 loss 0.3929903507232666 train acc 0.809952163061564\n",
      "epoch 5 batch id 801 loss 0.3109658360481262 train acc 0.8103542446941323\n",
      "epoch 5 batch id 1001 loss 0.482855886220932 train acc 0.8101273726273727\n",
      "epoch 5 batch id 1201 loss 0.5403850674629211 train acc 0.8087791423813488\n",
      "epoch 5 batch id 1401 loss 0.5705912709236145 train acc 0.810581727337616\n",
      "epoch 5 batch id 1601 loss 0.5750877857208252 train acc 0.8117387570268582\n",
      "epoch 5 batch id 1801 loss 0.7083356380462646 train acc 0.8139748750694059\n",
      "epoch 5 batch id 2001 loss 0.5146958827972412 train acc 0.8151861569215393\n",
      "epoch 5 batch id 2201 loss 0.7348799705505371 train acc 0.8163050885960926\n",
      "epoch 5 batch id 2401 loss 0.22743292152881622 train acc 0.8178623490212411\n",
      "epoch 5 batch id 2601 loss 0.5493189692497253 train acc 0.819156093810073\n",
      "epoch 5 batch id 2801 loss 0.34138596057891846 train acc 0.8209679578721885\n",
      "epoch 5 batch id 3001 loss 0.13007932901382446 train acc 0.8228090636454515\n",
      "epoch 5 batch id 3201 loss 0.3846987783908844 train acc 0.8236586223055296\n",
      "epoch 5 batch id 3401 loss 0.4453113377094269 train acc 0.8245277124375183\n",
      "epoch 5 batch id 3601 loss 0.30491626262664795 train acc 0.8251961260760899\n",
      "epoch 5 train acc 0.825426944971537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15e9d1b68a54b8c8418c10f7b2e9647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6894637053087758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaddf1c32fe4320a572bdeeff43969c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.14960554242134094 train acc 0.9375\n",
      "epoch 6 batch id 201 loss 0.48318779468536377 train acc 0.8453047263681592\n",
      "epoch 6 batch id 401 loss 0.6153068542480469 train acc 0.8456982543640897\n",
      "epoch 6 batch id 601 loss 0.3146199882030487 train acc 0.8439059900166389\n",
      "epoch 6 batch id 801 loss 0.24677644670009613 train acc 0.8424625468164794\n",
      "epoch 6 batch id 1001 loss 0.37980782985687256 train acc 0.8417207792207793\n",
      "epoch 6 batch id 1201 loss 0.5122339725494385 train acc 0.8408878018318068\n",
      "epoch 6 batch id 1401 loss 0.6184111833572388 train acc 0.8423001427551748\n",
      "epoch 6 batch id 1601 loss 0.557496964931488 train acc 0.8433791380387258\n",
      "epoch 6 batch id 1801 loss 0.8720369338989258 train acc 0.8443920044419767\n",
      "epoch 6 batch id 2001 loss 0.4535840153694153 train acc 0.8459676411794103\n",
      "epoch 6 batch id 2201 loss 0.5675052404403687 train acc 0.846830985915493\n",
      "epoch 6 batch id 2401 loss 0.1876327395439148 train acc 0.8478889004581425\n",
      "epoch 6 batch id 2601 loss 0.41545432806015015 train acc 0.8486399461745483\n",
      "epoch 6 batch id 2801 loss 0.29456308484077454 train acc 0.8502432167083185\n",
      "epoch 6 batch id 3001 loss 0.11231451481580734 train acc 0.8508934521826058\n",
      "epoch 6 batch id 3201 loss 0.33245813846588135 train acc 0.8514331458919088\n",
      "epoch 6 batch id 3401 loss 0.276308536529541 train acc 0.8515510144075272\n",
      "epoch 6 batch id 3601 loss 0.27630308270454407 train acc 0.8514128019994446\n",
      "epoch 6 train acc 0.8517298048251559\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb6fe8e43ac4eef9e4b8a011acf9b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.6869582881906826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff1c64449d843ecade104ae0cccd96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.16904717683792114 train acc 0.9375\n",
      "epoch 7 batch id 201 loss 0.5248503684997559 train acc 0.8535447761194029\n",
      "epoch 7 batch id 401 loss 0.5631686449050903 train acc 0.8583229426433915\n",
      "epoch 7 batch id 601 loss 0.23955106735229492 train acc 0.8577891014975042\n",
      "epoch 7 batch id 801 loss 0.21588607132434845 train acc 0.8588483146067416\n",
      "epoch 7 batch id 1001 loss 0.3903769850730896 train acc 0.8575487012987013\n",
      "epoch 7 batch id 1201 loss 0.39481863379478455 train acc 0.8565258118234804\n",
      "epoch 7 batch id 1401 loss 0.42969661951065063 train acc 0.8575347965738758\n",
      "epoch 7 batch id 1601 loss 0.6024450659751892 train acc 0.8580379450343535\n",
      "epoch 7 batch id 1801 loss 0.5603017807006836 train acc 0.8592448639644642\n",
      "epoch 7 batch id 2001 loss 0.33036181330680847 train acc 0.8606009495252374\n",
      "epoch 7 batch id 2201 loss 0.4992963671684265 train acc 0.8608445024988641\n",
      "epoch 7 batch id 2401 loss 0.17254726588726044 train acc 0.8616201582673886\n",
      "epoch 7 batch id 2601 loss 0.6182688474655151 train acc 0.8622524990388312\n",
      "epoch 7 batch id 2801 loss 0.1597333550453186 train acc 0.8634639414494824\n",
      "epoch 7 batch id 3001 loss 0.10873356461524963 train acc 0.8642431689436855\n",
      "epoch 7 batch id 3201 loss 0.37482553720474243 train acc 0.8646614339268979\n",
      "epoch 7 batch id 3401 loss 0.3632618188858032 train acc 0.8645343281387827\n",
      "epoch 7 batch id 3601 loss 0.3162887692451477 train acc 0.8644560538739239\n",
      "epoch 7 train acc 0.8647414611005693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81e611884f5451bb11671f290e3f5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.6858410075839654\n",
      "3190.03125 633.03125\n"
     ]
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy = [], []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_accuarcy.append(train_acc / (batch_id+1))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    test_accuarcy.append(test_acc / (batch_id+1))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    \n",
    "print(train_accuarcy, test_accuarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147525, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v5-bt.csv', index_col=0)\n",
    "\n",
    "train_set = train_set.dropna()\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "# 데이터 프레임 셔플\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 7  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(float(i[label_idx])) for i in dataset]         # self.labels = [np.int32(i[label_idx]) for i in dataset] 이꺼 변경\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류\n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2801a035d8a244cf95948a6ab0d95250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.8258147239685059 train acc 0.15625\n",
      "epoch 1 batch id 201 loss 1.6981139183044434 train acc 0.1957400497512438\n",
      "epoch 1 batch id 401 loss 1.7189544439315796 train acc 0.24251870324189526\n",
      "epoch 1 batch id 601 loss 1.6414874792099 train acc 0.2849417637271215\n",
      "epoch 1 batch id 801 loss 1.4733942747116089 train acc 0.325374531835206\n",
      "epoch 1 batch id 1001 loss 1.1684176921844482 train acc 0.35967157842157843\n",
      "epoch 1 batch id 1201 loss 1.3892159461975098 train acc 0.38782785179017487\n",
      "epoch 1 batch id 1401 loss 1.0611392259597778 train acc 0.41303087080656675\n",
      "epoch 1 batch id 1601 loss 1.136086106300354 train acc 0.4338694565896315\n",
      "epoch 1 batch id 1801 loss 1.4218536615371704 train acc 0.45164144919489174\n",
      "epoch 1 batch id 2001 loss 1.2108484506607056 train acc 0.46842203898050977\n",
      "epoch 1 batch id 2201 loss 0.9725021719932556 train acc 0.48225238527941844\n",
      "epoch 1 batch id 2401 loss 1.0816774368286133 train acc 0.4947808204914619\n",
      "epoch 1 batch id 2601 loss 0.9074342250823975 train acc 0.5063437139561707\n",
      "epoch 1 batch id 2801 loss 0.932909369468689 train acc 0.5163334523384505\n",
      "epoch 1 batch id 3001 loss 1.0308259725570679 train acc 0.524950016661113\n",
      "epoch 1 batch id 3201 loss 1.1810978651046753 train acc 0.5327143861293346\n",
      "epoch 1 batch id 3401 loss 0.9037482142448425 train acc 0.540034181123199\n",
      "epoch 1 batch id 3601 loss 0.7281855940818787 train acc 0.546254512635379\n",
      "epoch 1 train acc 0.5489123068582271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8750eb5eaf1f4c7d83cd7a827d929430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6603805525460456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7dba22af02414f8a9619b6169957f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.176632046699524 train acc 0.625\n",
      "epoch 2 batch id 201 loss 1.1063756942749023 train acc 0.6707089552238806\n",
      "epoch 2 batch id 401 loss 0.8295395374298096 train acc 0.6668485037406484\n",
      "epoch 2 batch id 601 loss 0.9796239137649536 train acc 0.6674292845257903\n",
      "epoch 2 batch id 801 loss 0.7904459834098816 train acc 0.6667837078651685\n",
      "epoch 2 batch id 1001 loss 0.7670003771781921 train acc 0.6672702297702298\n",
      "epoch 2 batch id 1201 loss 0.9879732728004456 train acc 0.6667360532889259\n",
      "epoch 2 batch id 1401 loss 0.7992398142814636 train acc 0.669187187723055\n",
      "epoch 2 batch id 1601 loss 0.9201534986495972 train acc 0.6703622735790131\n",
      "epoch 2 batch id 1801 loss 1.1080957651138306 train acc 0.6709987506940589\n",
      "epoch 2 batch id 2001 loss 1.105879545211792 train acc 0.6728979260369815\n",
      "epoch 2 batch id 2201 loss 0.6900357007980347 train acc 0.6743383689232167\n",
      "epoch 2 batch id 2401 loss 0.964436411857605 train acc 0.675747084548105\n",
      "epoch 2 batch id 2601 loss 0.7826902866363525 train acc 0.6778402537485583\n",
      "epoch 2 batch id 2801 loss 0.8310526609420776 train acc 0.6795564084255623\n",
      "epoch 2 batch id 3001 loss 0.9306880831718445 train acc 0.6808980339886704\n",
      "epoch 2 batch id 3201 loss 1.0102686882019043 train acc 0.6820915338956576\n",
      "epoch 2 batch id 3401 loss 0.7980865836143494 train acc 0.6834938253454866\n",
      "epoch 2 batch id 3601 loss 0.5693836212158203 train acc 0.6843237989447376\n",
      "epoch 2 train acc 0.6846960558416915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44201768b7b94a0b90840f8c34da7a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6875338569880823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aafe350706f4a3e8fd647202f3a1328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 1.070995807647705 train acc 0.59375\n",
      "epoch 3 batch id 201 loss 0.9139941334724426 train acc 0.7168843283582089\n",
      "epoch 3 batch id 401 loss 0.7532325387001038 train acc 0.7097100997506235\n",
      "epoch 3 batch id 601 loss 0.8690445423126221 train acc 0.7094425956738769\n",
      "epoch 3 batch id 801 loss 0.7896623015403748 train acc 0.7088014981273408\n",
      "epoch 3 batch id 1001 loss 0.5803099870681763 train acc 0.7084165834165834\n",
      "epoch 3 batch id 1201 loss 0.8147688508033752 train acc 0.7070930474604497\n",
      "epoch 3 batch id 1401 loss 0.7232925295829773 train acc 0.7083110278372591\n",
      "epoch 3 batch id 1601 loss 0.697036623954773 train acc 0.7098297938788257\n",
      "epoch 3 batch id 1801 loss 1.0439226627349854 train acc 0.7103692393114936\n",
      "epoch 3 batch id 2001 loss 0.9729917645454407 train acc 0.711987756121939\n",
      "epoch 3 batch id 2201 loss 0.5928052663803101 train acc 0.7135818945933666\n",
      "epoch 3 batch id 2401 loss 0.9389228820800781 train acc 0.7147802998750521\n",
      "epoch 3 batch id 2601 loss 0.7642312049865723 train acc 0.7164191657054979\n",
      "epoch 3 batch id 2801 loss 0.6911138892173767 train acc 0.7174892895394502\n",
      "epoch 3 batch id 3001 loss 0.884833574295044 train acc 0.718437604131956\n",
      "epoch 3 batch id 3201 loss 0.9762518405914307 train acc 0.7193943298969072\n",
      "epoch 3 batch id 3401 loss 0.765017569065094 train acc 0.7206612025874742\n",
      "epoch 3 batch id 3601 loss 0.45859819650650024 train acc 0.7218480977506249\n",
      "epoch 3 train acc 0.722011385199241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a95630d78640d8bb790c3163952690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6896329902491874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5082bd02f63b42dcadc1f05b03833759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.8964531421661377 train acc 0.6875\n",
      "epoch 4 batch id 201 loss 0.8608889579772949 train acc 0.7482898009950248\n",
      "epoch 4 batch id 401 loss 0.5816807150840759 train acc 0.7436876558603491\n",
      "epoch 4 batch id 601 loss 0.7982286214828491 train acc 0.7428244592346089\n",
      "epoch 4 batch id 801 loss 0.5978367924690247 train acc 0.742626404494382\n",
      "epoch 4 batch id 1001 loss 0.47371113300323486 train acc 0.7426323676323676\n",
      "epoch 4 batch id 1201 loss 0.7542970180511475 train acc 0.7413093255620317\n",
      "epoch 4 batch id 1401 loss 0.5904650092124939 train acc 0.7424830478229836\n",
      "epoch 4 batch id 1601 loss 0.5804026126861572 train acc 0.7437343847595252\n",
      "epoch 4 batch id 1801 loss 0.9444020390510559 train acc 0.7442566629650195\n",
      "epoch 4 batch id 2001 loss 0.7690519690513611 train acc 0.7456896551724138\n",
      "epoch 4 batch id 2201 loss 0.4693196713924408 train acc 0.7467202407996365\n",
      "epoch 4 batch id 2401 loss 0.9110593795776367 train acc 0.7475661182840483\n",
      "epoch 4 batch id 2601 loss 0.5821666717529297 train acc 0.7487865244136871\n",
      "epoch 4 batch id 2801 loss 0.6695132255554199 train acc 0.7498438057836487\n",
      "epoch 4 batch id 3001 loss 0.85884028673172 train acc 0.7505831389536821\n",
      "epoch 4 batch id 3201 loss 0.995989203453064 train acc 0.7512496094970321\n",
      "epoch 4 batch id 3401 loss 0.6261144280433655 train acc 0.7520949720670391\n",
      "epoch 4 batch id 3601 loss 0.39643779397010803 train acc 0.7526121216328797\n",
      "epoch 4 train acc 0.7529903090268365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d79e7ff0ae4959a5708f5a0e056ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6885495666305526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dc1673743c43e480370a0d81d79e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.8551768064498901 train acc 0.71875\n",
      "epoch 5 batch id 201 loss 0.7650254964828491 train acc 0.7744092039800995\n",
      "epoch 5 batch id 401 loss 0.6136534810066223 train acc 0.7707294264339152\n",
      "epoch 5 batch id 601 loss 0.6888339519500732 train acc 0.7704866888519135\n",
      "epoch 5 batch id 801 loss 0.5753196477890015 train acc 0.7704822097378277\n",
      "epoch 5 batch id 1001 loss 0.5074880123138428 train acc 0.7700736763236763\n",
      "epoch 5 batch id 1201 loss 0.6819635033607483 train acc 0.7686823480432973\n",
      "epoch 5 batch id 1401 loss 0.5957227349281311 train acc 0.7699188079942898\n",
      "epoch 5 batch id 1601 loss 0.5524907112121582 train acc 0.7705535602748282\n",
      "epoch 5 batch id 1801 loss 0.9223369359970093 train acc 0.7710820377568017\n",
      "epoch 5 batch id 2001 loss 0.7469671368598938 train acc 0.7726761619190404\n",
      "epoch 5 batch id 2201 loss 0.39328014850616455 train acc 0.7734268514311676\n",
      "epoch 5 batch id 2401 loss 0.811969518661499 train acc 0.7739093086214077\n",
      "epoch 5 batch id 2601 loss 0.5500221848487854 train acc 0.775254709727028\n",
      "epoch 5 batch id 2801 loss 0.5758482813835144 train acc 0.776095590860407\n",
      "epoch 5 batch id 3001 loss 0.7264533638954163 train acc 0.7768556314561813\n",
      "epoch 5 batch id 3201 loss 0.9117674231529236 train acc 0.7773352077475789\n",
      "epoch 5 batch id 3401 loss 0.5695158243179321 train acc 0.7778502646280506\n",
      "epoch 5 batch id 3601 loss 0.3344913721084595 train acc 0.7780390863648986\n",
      "epoch 5 train acc 0.7781749796692871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e196d9b7367407387b7c11637837ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6914274106175514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93e52ba6c324edc90b8e538fd5f889c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.6505228281021118 train acc 0.8125\n",
      "epoch 6 batch id 201 loss 0.6533713936805725 train acc 0.8003731343283582\n",
      "epoch 6 batch id 401 loss 0.5429118275642395 train acc 0.796290523690773\n",
      "epoch 6 batch id 601 loss 0.5808356404304504 train acc 0.7940411813643927\n",
      "epoch 6 batch id 801 loss 0.538279116153717 train acc 0.7933052434456929\n",
      "epoch 6 batch id 1001 loss 0.4904448688030243 train acc 0.7917082917082917\n",
      "epoch 6 batch id 1201 loss 0.668642520904541 train acc 0.7892381348875936\n",
      "epoch 6 batch id 1401 loss 0.49535423517227173 train acc 0.7903729478943612\n",
      "epoch 6 batch id 1601 loss 0.3651430904865265 train acc 0.790365396627108\n",
      "epoch 6 batch id 1801 loss 0.862176775932312 train acc 0.7904636313159356\n",
      "epoch 6 batch id 2001 loss 0.6120594143867493 train acc 0.7909638930534733\n",
      "epoch 6 batch id 2201 loss 0.38821980357170105 train acc 0.7917139936392549\n",
      "epoch 6 batch id 2401 loss 0.7008697986602783 train acc 0.792182944606414\n",
      "epoch 6 batch id 2601 loss 0.5514791011810303 train acc 0.7932405805459438\n",
      "epoch 6 batch id 2801 loss 0.4874771535396576 train acc 0.7939240449839343\n",
      "epoch 6 batch id 3001 loss 0.6821146011352539 train acc 0.7946101299566811\n",
      "epoch 6 batch id 3201 loss 0.7577506303787231 train acc 0.794614964073727\n",
      "epoch 6 batch id 3401 loss 0.43514081835746765 train acc 0.7951429726551015\n",
      "epoch 6 batch id 3601 loss 0.3662795126438141 train acc 0.7950222160510969\n",
      "epoch 6 train acc 0.7951511249661155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cdf95abd844dbf9aac616f3a70b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.6914612676056338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ab156e85e344468050a4dd2785df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.6601365804672241 train acc 0.75\n",
      "epoch 7 batch id 201 loss 0.6161670684814453 train acc 0.8022388059701493\n",
      "epoch 7 batch id 401 loss 0.5875775814056396 train acc 0.7997194513715711\n",
      "epoch 7 batch id 601 loss 0.6128020286560059 train acc 0.7995528286189684\n",
      "epoch 7 batch id 801 loss 0.3814089298248291 train acc 0.7992743445692884\n",
      "epoch 7 batch id 1001 loss 0.3913452625274658 train acc 0.7988573926073926\n",
      "epoch 7 batch id 1201 loss 0.7159830927848816 train acc 0.7969660699417153\n",
      "epoch 7 batch id 1401 loss 0.459958553314209 train acc 0.7977560670949322\n",
      "epoch 7 batch id 1601 loss 0.4526872932910919 train acc 0.7987781074328545\n",
      "epoch 7 batch id 1801 loss 0.8124911189079285 train acc 0.7989137978900611\n",
      "epoch 7 batch id 2001 loss 0.5914962291717529 train acc 0.7997876061969016\n",
      "epoch 7 batch id 2201 loss 0.37870484590530396 train acc 0.8002896410722399\n",
      "epoch 7 batch id 2401 loss 0.6466255784034729 train acc 0.8005648688046647\n",
      "epoch 7 batch id 2601 loss 0.47367438673973083 train acc 0.8012302960399846\n",
      "epoch 7 batch id 2801 loss 0.46107152104377747 train acc 0.8017560692609782\n",
      "epoch 7 batch id 3001 loss 0.6518225073814392 train acc 0.8022950683105632\n",
      "epoch 7 batch id 3201 loss 0.8571270108222961 train acc 0.8023469228366136\n",
      "epoch 7 batch id 3401 loss 0.5852366089820862 train acc 0.8025856365774772\n",
      "epoch 7 batch id 3601 loss 0.3564028739929199 train acc 0.8026329491807831\n",
      "epoch 7 train acc 0.8026819598807264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d285fa15384b8ebe35bb242836e474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.6932218309859155\n",
      "[0.5489123068582271, 0.6846960558416915, 0.722011385199241, 0.7529903090268365, 0.7781749796692871, 0.7951511249661155, 0.8026819598807264] [0.6603805525460456, 0.6875338569880823, 0.6896329902491874, 0.6885495666305526, 0.6914274106175514, 0.6914612676056338, 0.6932218309859155]\n"
     ]
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy = [], []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_accuarcy.append(train_acc / (batch_id+1))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    test_accuarcy.append(test_acc / (batch_id+1))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    \n",
    "print(train_accuarcy, test_accuarcy, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8073eded02c44df2b7299a9d98d47560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.9032812118530273 train acc 0.0625\n",
      "epoch 1 batch id 201 loss 1.751467227935791 train acc 0.17273009950248755\n",
      "epoch 1 batch id 401 loss 1.6798760890960693 train acc 0.23986907730673318\n",
      "epoch 1 batch id 601 loss 1.4346672296524048 train acc 0.2873336106489185\n",
      "epoch 1 batch id 801 loss 1.0671095848083496 train acc 0.33941947565543074\n",
      "epoch 1 batch id 1001 loss 1.075100064277649 train acc 0.37609265734265734\n",
      "epoch 1 batch id 1201 loss 1.2916501760482788 train acc 0.40825353871773523\n",
      "epoch 1 batch id 1401 loss 1.2795116901397705 train acc 0.4351133119200571\n",
      "epoch 1 batch id 1601 loss 0.9827401041984558 train acc 0.4555356027482823\n",
      "epoch 1 batch id 1801 loss 0.8462985157966614 train acc 0.47494447529150474\n",
      "epoch 1 batch id 2001 loss 0.9094249606132507 train acc 0.49094202898550726\n",
      "epoch 1 batch id 2201 loss 0.8161762952804565 train acc 0.5043588141753749\n",
      "epoch 1 batch id 2401 loss 0.8118577599525452 train acc 0.517219387755102\n",
      "epoch 1 batch id 2601 loss 0.6957474946975708 train acc 0.5273212226066898\n",
      "epoch 1 batch id 2801 loss 0.9265087842941284 train acc 0.5363039985719386\n",
      "epoch 1 batch id 3001 loss 1.2146188020706177 train acc 0.5441727757414195\n",
      "epoch 1 batch id 3201 loss 1.0593656301498413 train acc 0.5515463917525774\n",
      "epoch 1 batch id 3401 loss 1.162416934967041 train acc 0.556683695971773\n",
      "epoch 1 batch id 3601 loss 1.1809091567993164 train acc 0.5621702304915301\n",
      "epoch 1 batch id 3801 loss 1.0105693340301514 train acc 0.5682057353328072\n",
      "epoch 1 batch id 4001 loss 0.9134119153022766 train acc 0.5723178580354912\n",
      "epoch 1 train acc 0.5756804435483871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a018b26239ca4be2baf3386c976ed240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.22 GiB already allocated; 7.56 MiB free; 4.50 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 181\u001b[0m\n\u001b[0;32m    179\u001b[0m     valid_length\u001b[39m=\u001b[39m valid_length\n\u001b[0;32m    180\u001b[0m     label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 181\u001b[0m     out \u001b[39m=\u001b[39m model(token_ids, valid_length, segment_ids)\n\u001b[0;32m    182\u001b[0m     test_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calc_accuracy(out, label)\n\u001b[0;32m    183\u001b[0m test_accuarcy\u001b[39m.\u001b[39mappend(test_acc \u001b[39m/\u001b[39m (batch_id\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m, in \u001b[0;36mBERTClassifier.forward\u001b[1;34m(self, token_ids, valid_length, segment_ids)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, token_ids, valid_length, segment_ids):\n\u001b[0;32m     94\u001b[0m     attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_attention_mask(token_ids, valid_length)\n\u001b[1;32m---> 96\u001b[0m     _, pooler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids \u001b[39m=\u001b[39;49m token_ids, token_type_ids \u001b[39m=\u001b[39;49m segment_ids\u001b[39m.\u001b[39;49mlong(), attention_mask \u001b[39m=\u001b[39;49m attention_mask\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(token_ids\u001b[39m.\u001b[39;49mdevice))\n\u001b[0;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdr_rate:\n\u001b[0;32m     98\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooler)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:991\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    982\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    984\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    985\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    986\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    989\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    990\u001b[0m )\n\u001b[1;32m--> 991\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    992\u001b[0m     embedding_output,\n\u001b[0;32m    993\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    994\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    995\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    996\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    997\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    998\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    999\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1000\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1001\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1003\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1004\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    573\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    574\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    575\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[0;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    583\u001b[0m         hidden_states,\n\u001b[0;32m    584\u001b[0m         attention_mask,\n\u001b[0;32m    585\u001b[0m         layer_head_mask,\n\u001b[0;32m    586\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    587\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    588\u001b[0m         past_key_value,\n\u001b[0;32m    589\u001b[0m         output_attentions,\n\u001b[0;32m    590\u001b[0m     )\n\u001b[0;32m    592\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:470\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    459\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    460\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m ):\n\u001b[0;32m    468\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    471\u001b[0m         hidden_states,\n\u001b[0;32m    472\u001b[0m         attention_mask,\n\u001b[0;32m    473\u001b[0m         head_mask,\n\u001b[0;32m    474\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    475\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    476\u001b[0m     )\n\u001b[0;32m    477\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    479\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:401\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m ):\n\u001b[1;32m--> 401\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    402\u001b[0m         hidden_states,\n\u001b[0;32m    403\u001b[0m         attention_mask,\n\u001b[0;32m    404\u001b[0m         head_mask,\n\u001b[0;32m    405\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    406\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    407\u001b[0m         past_key_value,\n\u001b[0;32m    408\u001b[0m         output_attentions,\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    411\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:290\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 290\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[0;32m    292\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder:\n\u001b[0;32m    295\u001b[0m     \u001b[39m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[39m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[39m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     \u001b[39m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\modules\\linear.py:94\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\torch\\nn\\functional.py:1753\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m   1752\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1753\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.22 GiB already allocated; 7.56 MiB free; 4.50 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v5.csv', index_col=0)\n",
    "\n",
    "train_set = train_set.dropna()\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "# 데이터 프레임 셔플\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "train_set.shape\n",
    "\n",
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 7  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(float(i[label_idx])) for i in dataset]         # self.labels = [np.int32(i[label_idx]) for i in dataset] 이꺼 변경\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류\n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.1, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer\n",
    "\n",
    "train_accuarcy, test_accuarcy = [], []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_accuarcy.append(train_acc / (batch_id+1))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    test_accuarcy.append(test_acc / (batch_id+1))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    \n",
    "train_accuarcy, test_accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model/kobert-v10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 사이즈 확인(파라미터는 v1과 동일)\n",
    "import os\n",
    "\n",
    "model_path3 = 'model/kobert-v3.pt'\n",
    "size2 = os.path.getsize(model_path3) / (1024*1024) # mb 단위\n",
    "print(f\"Model size: {size2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
