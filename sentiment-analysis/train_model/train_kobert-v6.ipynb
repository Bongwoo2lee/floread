{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 런타임 애러 방지\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 30 02:00:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 457.51       Driver Version: 457.51       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 166... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   48C    P8    11W / 130W |    606MiB /  6144MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       748    C+G   ...oft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A      1212    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3948    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7632    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7656    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      9140    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9936    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10856    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11336    C+G   ...icrosoft.Media.Player.exe    N/A      |\n",
      "|    0   N/A  N/A     11980    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12596    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13656    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     14368    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     16712    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11628</th>\n",
       "      <td>이러나 저러나 집에서 지금 하는 일이 짱이예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115211</th>\n",
       "      <td>근데 연상녀 좋아하고있더라고요?..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112483</th>\n",
       "      <td>친구가 무사히 출산을 했다고 알려왔는데 얼마나 다행인지 몰라.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>왠만하면 걸치고 있는 코트 벗으시지?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132196</th>\n",
       "      <td>자식들이 아무도 나를 도와주지 않아서 슬퍼.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  sentence  emotion\n",
       "11628           이러나 저러나 집에서 지금 하는 일이 짱이예요.        0\n",
       "115211                 근데 연상녀 좋아하고있더라고요?..        1\n",
       "112483  친구가 무사히 출산을 했다고 알려왔는데 얼마나 다행인지 몰라.        0\n",
       "5428                 왠만하면 걸치고 있는 코트 벗으시지?         5\n",
       "132196            자식들이 아무도 나를 도와주지 않아서 슬퍼.        2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v3.csv', index_col=0) \n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__learning rate = e-5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 8  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f5df1840a94c3ba9124f0e47537838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.8971830606460571 train acc 0.15625\n",
      "epoch 1 batch id 201 loss 1.8041564226150513 train acc 0.14894278606965175\n",
      "epoch 1 batch id 401 loss 1.785013198852539 train acc 0.17573254364089774\n",
      "epoch 1 batch id 601 loss 1.5961964130401611 train acc 0.22753743760399334\n",
      "epoch 1 batch id 801 loss 1.3921833038330078 train acc 0.27071629213483145\n",
      "epoch 1 batch id 1001 loss 1.238572597503662 train acc 0.3160277222777223\n",
      "epoch 1 batch id 1201 loss 1.423027753829956 train acc 0.3473667776852623\n",
      "epoch 1 batch id 1401 loss 1.1432969570159912 train acc 0.37517844396859384\n",
      "epoch 1 batch id 1601 loss 1.0395294427871704 train acc 0.39789584634603375\n",
      "epoch 1 batch id 1801 loss 1.2298238277435303 train acc 0.41837867851193783\n",
      "epoch 1 batch id 2001 loss 1.1632047891616821 train acc 0.4368753123438281\n",
      "epoch 1 batch id 2201 loss 1.0964641571044922 train acc 0.454253748296229\n",
      "epoch 1 batch id 2401 loss 1.249979019165039 train acc 0.46889316951270305\n",
      "epoch 1 batch id 2601 loss 1.2147047519683838 train acc 0.48150951557093424\n",
      "epoch 1 batch id 2801 loss 0.767318069934845 train acc 0.4922795430203499\n",
      "epoch 1 batch id 3001 loss 0.5988214015960693 train acc 0.5034988337220926\n",
      "epoch 1 batch id 3201 loss 0.6020339727401733 train acc 0.5121739300218682\n",
      "epoch 1 train acc 0.5203424153166422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459dbc82bb1a4c448291364f80b734a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6599916141752445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95da4ca89b3c42eba9b9cd43b1136fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.9081122875213623 train acc 0.6875\n",
      "epoch 2 batch id 201 loss 1.1028437614440918 train acc 0.6553171641791045\n",
      "epoch 2 batch id 401 loss 0.9093326330184937 train acc 0.6530548628428927\n",
      "epoch 2 batch id 601 loss 0.9103515148162842 train acc 0.6543261231281198\n",
      "epoch 2 batch id 801 loss 0.9869348406791687 train acc 0.6522705992509363\n",
      "epoch 2 batch id 1001 loss 0.9001228213310242 train acc 0.6564373126873126\n",
      "epoch 2 batch id 1201 loss 1.1010137796401978 train acc 0.6572907993338885\n",
      "epoch 2 batch id 1401 loss 1.019248366355896 train acc 0.6592389364739472\n",
      "epoch 2 batch id 1601 loss 0.8049092292785645 train acc 0.6603099625234229\n",
      "epoch 2 batch id 1801 loss 1.0864335298538208 train acc 0.6614554414214325\n",
      "epoch 2 batch id 2001 loss 0.896577775478363 train acc 0.6612787356321839\n",
      "epoch 2 batch id 2201 loss 0.9312554597854614 train acc 0.663604611540209\n",
      "epoch 2 batch id 2401 loss 0.988960862159729 train acc 0.6655299875052062\n",
      "epoch 2 batch id 2601 loss 1.0572643280029297 train acc 0.6674716455209535\n",
      "epoch 2 batch id 2801 loss 0.7007271647453308 train acc 0.6688124776865405\n",
      "epoch 2 batch id 3001 loss 0.4966455101966858 train acc 0.6711825224925025\n",
      "epoch 2 batch id 3201 loss 0.5022573471069336 train acc 0.6722703842549204\n",
      "epoch 2 train acc 0.674251350024546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22a98648554413b89f3a5ffcd3021ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6813994725252215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae59e15f6914bd68faa0e031f958ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.8242672681808472 train acc 0.65625\n",
      "epoch 3 batch id 201 loss 1.0347223281860352 train acc 0.7027363184079602\n",
      "epoch 3 batch id 401 loss 0.7011292576789856 train acc 0.6984881546134664\n",
      "epoch 3 batch id 601 loss 0.8223734498023987 train acc 0.699563227953411\n",
      "epoch 3 batch id 801 loss 0.8989866971969604 train acc 0.69666822721598\n",
      "epoch 3 batch id 1001 loss 0.7903527021408081 train acc 0.6996128871128872\n",
      "epoch 3 batch id 1201 loss 1.05911123752594 train acc 0.6997554121565362\n",
      "epoch 3 batch id 1401 loss 1.1098003387451172 train acc 0.7002141327623126\n",
      "epoch 3 batch id 1601 loss 0.7144538164138794 train acc 0.7006168019987508\n",
      "epoch 3 batch id 1801 loss 0.8862524032592773 train acc 0.7007912270960578\n",
      "epoch 3 batch id 2001 loss 0.8934391736984253 train acc 0.7013212143928036\n",
      "epoch 3 batch id 2201 loss 0.8682677149772644 train acc 0.7031179009541118\n",
      "epoch 3 batch id 2401 loss 0.8805285692214966 train acc 0.7050317576009996\n",
      "epoch 3 batch id 2601 loss 0.9568201899528503 train acc 0.7062307766243753\n",
      "epoch 3 batch id 2801 loss 0.5188947319984436 train acc 0.7072808818279186\n",
      "epoch 3 batch id 3001 loss 0.5079919695854187 train acc 0.7095551482839053\n",
      "epoch 3 batch id 3201 loss 0.3938308656215668 train acc 0.7104615745079662\n",
      "epoch 3 train acc 0.7117973735886107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eada77ba5784f6184d20118b05da3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6859780688277769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5777a77d78794012857cb3ca9287ed45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.723929762840271 train acc 0.6875\n",
      "epoch 4 batch id 201 loss 1.0655529499053955 train acc 0.7377176616915423\n",
      "epoch 4 batch id 401 loss 0.5858783721923828 train acc 0.732076059850374\n",
      "epoch 4 batch id 601 loss 0.7162054777145386 train acc 0.7317491680532446\n",
      "epoch 4 batch id 801 loss 0.7998660802841187 train acc 0.7286594881398252\n",
      "epoch 4 batch id 1001 loss 0.6983622908592224 train acc 0.7307067932067932\n",
      "epoch 4 batch id 1201 loss 1.055549144744873 train acc 0.729990632805995\n",
      "epoch 4 batch id 1401 loss 1.0035539865493774 train acc 0.7311964668094219\n",
      "epoch 4 batch id 1601 loss 0.7015185356140137 train acc 0.7317887257963772\n",
      "epoch 4 batch id 1801 loss 0.8077678680419922 train acc 0.7321626873958912\n",
      "epoch 4 batch id 2001 loss 0.8292643427848816 train acc 0.7324775112443778\n",
      "epoch 4 batch id 2201 loss 0.7264562845230103 train acc 0.7349074284416175\n",
      "epoch 4 batch id 2401 loss 0.783591628074646 train acc 0.7361516034985423\n",
      "epoch 4 batch id 2601 loss 0.860729455947876 train acc 0.7372645136485967\n",
      "epoch 4 batch id 2801 loss 0.4410385191440582 train acc 0.7385866654766154\n",
      "epoch 4 batch id 3001 loss 0.3888426125049591 train acc 0.7405031656114629\n",
      "epoch 4 batch id 3201 loss 0.41363972425460815 train acc 0.7413503592627304\n",
      "epoch 4 train acc 0.7427988463426608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c969099e7144e787474ea3b9e3e405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.687928893327188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8bab450ec04bf7bc2b735151cce629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.6350523829460144 train acc 0.75\n",
      "epoch 5 batch id 201 loss 0.9100580811500549 train acc 0.7653917910447762\n",
      "epoch 5 batch id 401 loss 0.5264415740966797 train acc 0.7609881546134664\n",
      "epoch 5 batch id 601 loss 0.6460691690444946 train acc 0.7627391846921797\n",
      "epoch 5 batch id 801 loss 0.7493385672569275 train acc 0.7586610486891385\n",
      "epoch 5 batch id 1001 loss 0.7259968519210815 train acc 0.7605831668331668\n",
      "epoch 5 batch id 1201 loss 0.8377357721328735 train acc 0.7605901332223147\n",
      "epoch 5 batch id 1401 loss 0.8713979721069336 train acc 0.7609966095645967\n",
      "epoch 5 batch id 1601 loss 0.648465096950531 train acc 0.7618675827607745\n",
      "epoch 5 batch id 1801 loss 0.7071273922920227 train acc 0.7623715991116047\n",
      "epoch 5 batch id 2001 loss 0.8379148840904236 train acc 0.7628373313343328\n",
      "epoch 5 batch id 2201 loss 0.676481306552887 train acc 0.7639425261244889\n",
      "epoch 5 batch id 2401 loss 0.7456495761871338 train acc 0.7653451686797168\n",
      "epoch 5 batch id 2601 loss 0.8416330814361572 train acc 0.7660755478662054\n",
      "epoch 5 batch id 2801 loss 0.31493625044822693 train acc 0.7670809532309889\n",
      "epoch 5 batch id 3001 loss 0.36797648668289185 train acc 0.7688687104298567\n",
      "epoch 5 batch id 3201 loss 0.24253785610198975 train acc 0.7693689472039987\n",
      "epoch 5 train acc 0.7706737849779087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9cf4d9789646b0836c57582618cb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6911824012905208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d59f2dae70d4685adf36da8257a15d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.6014813780784607 train acc 0.78125\n",
      "epoch 6 batch id 201 loss 0.7500813603401184 train acc 0.7932213930348259\n",
      "epoch 6 batch id 401 loss 0.39017343521118164 train acc 0.7842113466334164\n",
      "epoch 6 batch id 601 loss 0.5756874084472656 train acc 0.7841618136439268\n",
      "epoch 6 batch id 801 loss 0.5853288173675537 train acc 0.7795724094881398\n",
      "epoch 6 batch id 1001 loss 0.5835893750190735 train acc 0.78252997002997\n",
      "epoch 6 batch id 1201 loss 0.717678964138031 train acc 0.7836178184845962\n",
      "epoch 6 batch id 1401 loss 0.6399487257003784 train acc 0.7837928265524625\n",
      "epoch 6 batch id 1601 loss 0.49559852480888367 train acc 0.7841778575890068\n",
      "epoch 6 batch id 1801 loss 0.7371392846107483 train acc 0.7843559133814547\n",
      "epoch 6 batch id 2001 loss 0.7329521775245667 train acc 0.7847482508745627\n",
      "epoch 6 batch id 2201 loss 0.6145915985107422 train acc 0.7862051340299864\n",
      "epoch 6 batch id 2401 loss 0.6567460894584656 train acc 0.7869507496876301\n",
      "epoch 6 batch id 2601 loss 0.8664721250534058 train acc 0.7879181084198386\n",
      "epoch 6 batch id 2801 loss 0.3183184862136841 train acc 0.7884684041413781\n",
      "epoch 6 batch id 3001 loss 0.3237275183200836 train acc 0.7897992335888038\n",
      "epoch 6 batch id 3201 loss 0.2127387672662735 train acc 0.7902120431115276\n",
      "epoch 6 train acc 0.791151202749141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1569f82a1585497f93a230655f986a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.6882825702872944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef234a7f6277477682a3b6da8bc06a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.5165911316871643 train acc 0.71875\n",
      "epoch 7 batch id 201 loss 0.9076423048973083 train acc 0.8041044776119403\n",
      "epoch 7 batch id 401 loss 0.38108810782432556 train acc 0.7995635910224439\n",
      "epoch 7 batch id 601 loss 0.5669339299201965 train acc 0.8011127287853578\n",
      "epoch 7 batch id 801 loss 0.5849937796592712 train acc 0.7975187265917603\n",
      "epoch 7 batch id 1001 loss 0.5727391839027405 train acc 0.8000124875124875\n",
      "epoch 7 batch id 1201 loss 0.8483688831329346 train acc 0.8003486677768527\n",
      "epoch 7 batch id 1401 loss 0.663503885269165 train acc 0.7999866167023555\n",
      "epoch 7 batch id 1601 loss 0.460993230342865 train acc 0.8000078076202374\n",
      "epoch 7 batch id 1801 loss 0.650044858455658 train acc 0.7990873126041088\n",
      "epoch 7 batch id 2001 loss 0.8157662749290466 train acc 0.7996939030484758\n",
      "epoch 7 batch id 2201 loss 0.5382829904556274 train acc 0.8005452067242163\n",
      "epoch 7 batch id 2401 loss 0.6988325715065002 train acc 0.8015019783423574\n",
      "epoch 7 batch id 2601 loss 0.5725100636482239 train acc 0.8022395232602845\n",
      "epoch 7 batch id 2801 loss 0.2625220715999603 train acc 0.8029163691538737\n",
      "epoch 7 batch id 3001 loss 0.31402501463890076 train acc 0.8042735754748417\n",
      "epoch 7 batch id 3201 loss 0.19474346935749054 train acc 0.8043775382692908\n",
      "epoch 7 train acc 0.8052190721649485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438e445f81074b57bdede4981bbae1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.6868470579198033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb4db28c9434a1bbb0af1ecd5c61051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.4622437357902527 train acc 0.875\n",
      "epoch 8 batch id 201 loss 0.7556312680244446 train acc 0.8215174129353234\n",
      "epoch 8 batch id 401 loss 0.5616306662559509 train acc 0.8144482543640897\n",
      "epoch 8 batch id 601 loss 0.5034376978874207 train acc 0.8138519134775375\n",
      "epoch 8 batch id 801 loss 0.6494439840316772 train acc 0.8105493133583022\n",
      "epoch 8 batch id 1001 loss 0.7208328247070312 train acc 0.8106580919080919\n",
      "epoch 8 batch id 1201 loss 0.7784857153892517 train acc 0.8110689009159034\n",
      "epoch 8 batch id 1401 loss 0.5861051678657532 train acc 0.8108940042826552\n",
      "epoch 8 batch id 1601 loss 0.6852321624755859 train acc 0.8097478138663335\n",
      "epoch 8 batch id 1801 loss 0.5190677642822266 train acc 0.8090123542476402\n",
      "epoch 8 batch id 2001 loss 0.8699840307235718 train acc 0.8094234132933533\n",
      "epoch 8 batch id 2201 loss 0.6189286708831787 train acc 0.8101857110404361\n",
      "epoch 8 batch id 2401 loss 0.6380034685134888 train acc 0.8110162432319866\n",
      "epoch 8 batch id 2601 loss 0.6871709823608398 train acc 0.811538831218762\n",
      "epoch 8 batch id 2801 loss 0.233245387673378 train acc 0.8118417529453766\n",
      "epoch 8 batch id 3001 loss 0.27061593532562256 train acc 0.8128644618460513\n",
      "epoch 8 batch id 3201 loss 0.20088477432727814 train acc 0.8126757263355201\n",
      "epoch 8 train acc 0.8131473981345115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4888381f3f4d5ca3c40db290d1223b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.6872519460234546\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률: e-5, 에포크: 5 )0.6911824012905208"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__learingrate = 5e-6__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 8  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-6\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525c95ee5f5c42e4901a32d9cf8c5d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.7560423612594604 train acc 0.25\n",
      "epoch 1 batch id 201 loss 1.8199613094329834 train acc 0.18921019900497513\n",
      "epoch 1 batch id 401 loss 1.7832614183425903 train acc 0.2214775561097257\n",
      "epoch 1 batch id 601 loss 1.6579742431640625 train acc 0.2599313643926789\n",
      "epoch 1 batch id 801 loss 1.55643892288208 train acc 0.2914715980024969\n",
      "epoch 1 batch id 1001 loss 1.4163613319396973 train acc 0.3179632867132867\n",
      "epoch 1 batch id 1201 loss 1.5955044031143188 train acc 0.3362822647793505\n",
      "epoch 1 batch id 1401 loss 1.3921822309494019 train acc 0.3553042469664525\n",
      "epoch 1 batch id 1601 loss 1.1526474952697754 train acc 0.37441442848219864\n",
      "epoch 1 batch id 1801 loss 1.3657439947128296 train acc 0.38988756246529704\n",
      "epoch 1 batch id 2001 loss 1.4022252559661865 train acc 0.40445402298850575\n",
      "epoch 1 batch id 2201 loss 1.257387638092041 train acc 0.4181763970922308\n",
      "epoch 1 batch id 2401 loss 1.2686065435409546 train acc 0.43157798833819244\n",
      "epoch 1 batch id 2601 loss 1.2067782878875732 train acc 0.44325499807766244\n",
      "epoch 1 batch id 2801 loss 1.080005168914795 train acc 0.4540119600142806\n",
      "epoch 1 batch id 3001 loss 0.692206859588623 train acc 0.46534488503832055\n",
      "epoch 1 batch id 3201 loss 0.708160400390625 train acc 0.47506638550452984\n",
      "epoch 1 train acc 0.48405436917034855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18231f0c62c74ea6846cf7934c5e014b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6482722640446561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7277190cd58b4a7e9b0f35af161848b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.0646320581436157 train acc 0.59375\n",
      "epoch 2 batch id 201 loss 1.1876798868179321 train acc 0.636660447761194\n",
      "epoch 2 batch id 401 loss 0.9579286575317383 train acc 0.6388715710723192\n",
      "epoch 2 batch id 601 loss 1.014765739440918 train acc 0.6410669717138103\n",
      "epoch 2 batch id 801 loss 1.0426069498062134 train acc 0.6399422596754057\n",
      "epoch 2 batch id 1001 loss 0.9033598899841309 train acc 0.6426073926073926\n",
      "epoch 2 batch id 1201 loss 1.0622522830963135 train acc 0.643526228143214\n",
      "epoch 2 batch id 1401 loss 1.0717408657073975 train acc 0.6459225553176302\n",
      "epoch 2 batch id 1601 loss 0.918716311454773 train acc 0.6458268269831355\n",
      "epoch 2 batch id 1801 loss 1.133937120437622 train acc 0.6463943642420877\n",
      "epoch 2 batch id 2001 loss 0.9892600774765015 train acc 0.6466454272863568\n",
      "epoch 2 batch id 2201 loss 1.029283046722412 train acc 0.648469445706497\n",
      "epoch 2 batch id 2401 loss 1.1141307353973389 train acc 0.6503800499791753\n",
      "epoch 2 batch id 2601 loss 1.1424827575683594 train acc 0.6517325067281815\n",
      "epoch 2 batch id 2801 loss 0.8330070376396179 train acc 0.6526017493752231\n",
      "epoch 2 batch id 3001 loss 0.5228561162948608 train acc 0.65468802065978\n",
      "epoch 2 batch id 3201 loss 0.45493608713150024 train acc 0.655400656044986\n",
      "epoch 2 train acc 0.6573576337751595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582c527d3b424732b1a4bbdc908de653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6742219106877656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf6319a78a14d0cb646c0ff5d73126f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.81449955701828 train acc 0.625\n",
      "epoch 3 batch id 201 loss 0.9909664988517761 train acc 0.681592039800995\n",
      "epoch 3 batch id 401 loss 0.8221098780632019 train acc 0.6781483790523691\n",
      "epoch 3 batch id 601 loss 0.9510586261749268 train acc 0.6808444259567388\n",
      "epoch 3 batch id 801 loss 0.9641091823577881 train acc 0.6794241573033708\n",
      "epoch 3 batch id 1001 loss 0.8540115356445312 train acc 0.6812562437562437\n",
      "epoch 3 batch id 1201 loss 0.9977259039878845 train acc 0.6802664446294754\n",
      "epoch 3 batch id 1401 loss 1.044524073600769 train acc 0.6820128479657388\n",
      "epoch 3 batch id 1601 loss 0.8528668284416199 train acc 0.6818199562773267\n",
      "epoch 3 batch id 1801 loss 1.0531636476516724 train acc 0.6822251526929484\n",
      "epoch 3 batch id 2001 loss 0.8951471447944641 train acc 0.6818153423288356\n",
      "epoch 3 batch id 2201 loss 0.9447652101516724 train acc 0.6829566106315311\n",
      "epoch 3 batch id 2401 loss 0.9381710290908813 train acc 0.6838817159516868\n",
      "epoch 3 batch id 2601 loss 1.0848124027252197 train acc 0.6841959823144944\n",
      "epoch 3 batch id 2801 loss 0.5990135669708252 train acc 0.6846661906461978\n",
      "epoch 3 batch id 3001 loss 0.45028215646743774 train acc 0.6864899200266578\n",
      "epoch 3 batch id 3201 loss 0.40753379464149475 train acc 0.6870606841611996\n",
      "epoch 3 train acc 0.6882271723122239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04794d800e6f4defa965623bd92cb4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6832030649869412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84600cfd995d42a5a0492792b562ac0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.7308766841888428 train acc 0.6875\n",
      "epoch 4 batch id 201 loss 1.0068799257278442 train acc 0.699160447761194\n",
      "epoch 4 batch id 401 loss 0.7843340635299683 train acc 0.6982543640897756\n",
      "epoch 4 batch id 601 loss 0.8583381772041321 train acc 0.699303244592346\n",
      "epoch 4 batch id 801 loss 0.9086947441101074 train acc 0.69666822721598\n",
      "epoch 4 batch id 1001 loss 0.8532354831695557 train acc 0.7002060439560439\n",
      "epoch 4 batch id 1201 loss 1.0290346145629883 train acc 0.7001457119067444\n",
      "epoch 4 batch id 1401 loss 0.9974351525306702 train acc 0.701686295503212\n",
      "epoch 4 batch id 1601 loss 0.7042068243026733 train acc 0.7016317926296065\n",
      "epoch 4 batch id 1801 loss 0.8527926206588745 train acc 0.701467934480844\n",
      "epoch 4 batch id 2001 loss 0.8598966002464294 train acc 0.7015086206896551\n",
      "epoch 4 batch id 2201 loss 0.8696739673614502 train acc 0.7030895047705589\n",
      "epoch 4 batch id 2401 loss 0.87723708152771 train acc 0.7042898792169929\n",
      "epoch 4 batch id 2601 loss 0.873131513595581 train acc 0.7048130526720492\n",
      "epoch 4 batch id 2801 loss 0.5899510383605957 train acc 0.705261513745091\n",
      "epoch 4 batch id 3001 loss 0.48864802718162537 train acc 0.7068789570143286\n",
      "epoch 4 batch id 3201 loss 0.42193329334259033 train acc 0.7074058887847547\n",
      "epoch 4 train acc 0.7087168630338734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c65020a0e64bf9878d1ba0ae9a273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6867878450350796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee27312e95d4aa58fcded07d824dd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.7460145950317383 train acc 0.65625\n",
      "epoch 5 batch id 201 loss 0.9557799696922302 train acc 0.7223258706467661\n",
      "epoch 5 batch id 401 loss 0.7206341028213501 train acc 0.7192955112219451\n",
      "epoch 5 batch id 601 loss 0.8167496919631958 train acc 0.718801996672213\n",
      "epoch 5 batch id 801 loss 0.8008030652999878 train acc 0.7170333957553059\n",
      "epoch 5 batch id 1001 loss 0.7334114909172058 train acc 0.7205919080919081\n",
      "epoch 5 batch id 1201 loss 0.9250078201293945 train acc 0.719738759367194\n",
      "epoch 5 batch id 1401 loss 1.0890610218048096 train acc 0.7208244111349036\n",
      "epoch 5 batch id 1601 loss 0.6852176785469055 train acc 0.7199016239850093\n",
      "epoch 5 batch id 1801 loss 1.0151104927062988 train acc 0.7197910882842865\n",
      "epoch 5 batch id 2001 loss 0.9826447367668152 train acc 0.7197651174412794\n",
      "epoch 5 batch id 2201 loss 0.8871434330940247 train acc 0.7221433439345752\n",
      "epoch 5 batch id 2401 loss 0.8034678101539612 train acc 0.7227717617659308\n",
      "epoch 5 batch id 2601 loss 0.9389786124229431 train acc 0.7231833910034602\n",
      "epoch 5 batch id 2801 loss 0.5538868308067322 train acc 0.7235250803284541\n",
      "epoch 5 batch id 3001 loss 0.45246952772140503 train acc 0.72507080973009\n",
      "epoch 5 batch id 3201 loss 0.41326332092285156 train acc 0.725642377382068\n",
      "epoch 5 train acc 0.7267857142857143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbb56d8df4c4f9381c523db9804568e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6883481845649613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6357d029b67147139837ba5e50f7dfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.7258897423744202 train acc 0.6875\n",
      "epoch 6 batch id 201 loss 0.9424387812614441 train acc 0.7388059701492538\n",
      "epoch 6 batch id 401 loss 0.6600649356842041 train acc 0.7340243142144638\n",
      "epoch 6 batch id 601 loss 0.753972589969635 train acc 0.7345049916805324\n",
      "epoch 6 batch id 801 loss 0.8334688544273376 train acc 0.7322877652933832\n",
      "epoch 6 batch id 1001 loss 0.7524935007095337 train acc 0.7343593906093906\n",
      "epoch 6 batch id 1201 loss 1.0087593793869019 train acc 0.7342579100749376\n",
      "epoch 6 batch id 1401 loss 0.8968778252601624 train acc 0.734943790149893\n",
      "epoch 6 batch id 1601 loss 0.7052469849586487 train acc 0.7348532167395377\n",
      "epoch 6 batch id 1801 loss 0.873069167137146 train acc 0.7342448639644642\n",
      "epoch 6 batch id 2001 loss 0.804807186126709 train acc 0.7338049725137431\n",
      "epoch 6 batch id 2201 loss 0.7369181513786316 train acc 0.7353475692866879\n",
      "epoch 6 batch id 2401 loss 0.9235896468162537 train acc 0.7359303415243649\n",
      "epoch 6 batch id 2601 loss 0.884548544883728 train acc 0.736159169550173\n",
      "epoch 6 batch id 2801 loss 0.5556133985519409 train acc 0.7362549089610854\n",
      "epoch 6 batch id 3001 loss 0.44584986567497253 train acc 0.7375249916694435\n",
      "epoch 6 batch id 3201 loss 0.4569159746170044 train acc 0.7376112933458294\n",
      "epoch 6 train acc 0.7385155866470299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632128e1686247aaa0c431ac65aef444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.6894748297229477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579da9925f6845ed8e4e6db66ae8d691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.6103459000587463 train acc 0.78125\n",
      "epoch 7 batch id 201 loss 0.912868857383728 train acc 0.746113184079602\n",
      "epoch 7 batch id 401 loss 0.6236191391944885 train acc 0.7429862842892768\n",
      "epoch 7 batch id 601 loss 0.7048535943031311 train acc 0.7444363560732113\n",
      "epoch 7 batch id 801 loss 0.8117244839668274 train acc 0.7413779650436954\n",
      "epoch 7 batch id 1001 loss 0.7018693685531616 train acc 0.7434440559440559\n",
      "epoch 7 batch id 1201 loss 0.9094683527946472 train acc 0.7438592839300583\n",
      "epoch 7 batch id 1401 loss 1.0306223630905151 train acc 0.7440444325481799\n",
      "epoch 7 batch id 1601 loss 0.7332769632339478 train acc 0.7428169893816364\n",
      "epoch 7 batch id 1801 loss 0.7370643615722656 train acc 0.742816490838423\n",
      "epoch 7 batch id 2001 loss 0.9353119730949402 train acc 0.7430503498250874\n",
      "epoch 7 batch id 2201 loss 0.7949662208557129 train acc 0.7439374148114494\n",
      "epoch 7 batch id 2401 loss 0.7995442748069763 train acc 0.744338296543107\n",
      "epoch 7 batch id 2601 loss 0.9155064225196838 train acc 0.7445693963860054\n",
      "epoch 7 batch id 2801 loss 0.42531415820121765 train acc 0.7444551053195287\n",
      "epoch 7 batch id 3001 loss 0.36221903562545776 train acc 0.7455119126957681\n",
      "epoch 7 batch id 3201 loss 0.3204815089702606 train acc 0.7459582942830365\n",
      "epoch 7 train acc 0.7467231222385862\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cb9cf0fc8147b7bc5d904d36b6e747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.6888346904286372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d0405a420e4f4fad7238a91d92baf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.6644065380096436 train acc 0.6875\n",
      "epoch 8 batch id 201 loss 0.758638322353363 train acc 0.7534203980099502\n",
      "epoch 8 batch id 401 loss 0.5922772884368896 train acc 0.7510910224438903\n",
      "epoch 8 batch id 601 loss 0.7285424470901489 train acc 0.7514039101497504\n",
      "epoch 8 batch id 801 loss 0.7126544117927551 train acc 0.7483614232209738\n",
      "epoch 8 batch id 1001 loss 0.7316358685493469 train acc 0.7499687812187812\n",
      "epoch 8 batch id 1201 loss 0.9857854247093201 train acc 0.7491933805162365\n",
      "epoch 8 batch id 1401 loss 0.8305946588516235 train acc 0.7493085296216988\n",
      "epoch 8 batch id 1601 loss 0.6849831342697144 train acc 0.748145690193629\n",
      "epoch 8 batch id 1801 loss 0.8749753832817078 train acc 0.7470502498611882\n",
      "epoch 8 batch id 2001 loss 0.7876958847045898 train acc 0.7469546476761619\n",
      "epoch 8 batch id 2201 loss 0.7933128476142883 train acc 0.7479696728759655\n",
      "epoch 8 batch id 2401 loss 0.6795426607131958 train acc 0.7486984589754269\n",
      "epoch 8 batch id 2601 loss 0.891927182674408 train acc 0.7488826412918108\n",
      "epoch 8 batch id 2801 loss 0.4260817766189575 train acc 0.7487727597286683\n",
      "epoch 8 batch id 3001 loss 0.4432443082332611 train acc 0.749531406197934\n",
      "epoch 8 batch id 3201 loss 0.3529123365879059 train acc 0.7494142455482662\n",
      "epoch 8 train acc 0.7501196612665685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481aec996a024b568ba8ead7c487f379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.6888714984380601\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률: 5e-6, 에포크: 6 ) 0.6894748297229477"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Learning rate = e-4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 6  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8c139102db4434a8c20c500916e77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.864007830619812 train acc 0.1875\n",
      "epoch 1 batch id 201 loss 1.5482573509216309 train acc 0.2751865671641791\n",
      "epoch 1 batch id 401 loss 1.2720866203308105 train acc 0.3870791770573566\n",
      "epoch 1 batch id 601 loss 1.0586458444595337 train acc 0.45211106489184694\n",
      "epoch 1 batch id 801 loss 1.0244446992874146 train acc 0.49020755305867664\n",
      "epoch 1 batch id 1001 loss 0.9156310558319092 train acc 0.5175761738261738\n",
      "epoch 1 batch id 1201 loss 1.1014080047607422 train acc 0.5339560782681099\n",
      "epoch 1 batch id 1401 loss 0.9878259897232056 train acc 0.5471538187009279\n",
      "epoch 1 batch id 1601 loss 0.9179600477218628 train acc 0.5560977514053717\n",
      "epoch 1 batch id 1801 loss 1.2706944942474365 train acc 0.5627255691282621\n",
      "epoch 1 batch id 2001 loss 0.9917345643043518 train acc 0.5679660169915043\n",
      "epoch 1 batch id 2201 loss 0.8570663332939148 train acc 0.5734609268514311\n",
      "epoch 1 batch id 2401 loss 1.1694954633712769 train acc 0.577741045397751\n",
      "epoch 1 batch id 2601 loss 0.9644126892089844 train acc 0.580642060745867\n",
      "epoch 1 batch id 2801 loss 0.8647170662879944 train acc 0.5835862192074259\n",
      "epoch 1 batch id 3001 loss 0.6163833737373352 train acc 0.5871896867710763\n",
      "epoch 1 batch id 3201 loss 0.7970173954963684 train acc 0.5896887691346454\n",
      "epoch 1 train acc 0.5926086156111929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab0549e307d4e98b55ed371e6acdf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.656907743124904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614b4290e6954744a68e98ace6b8421e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.9108739495277405 train acc 0.625\n",
      "epoch 2 batch id 201 loss 1.089524745941162 train acc 0.6414800995024875\n",
      "epoch 2 batch id 401 loss 0.8243495225906372 train acc 0.640819825436409\n",
      "epoch 2 batch id 601 loss 0.8856736421585083 train acc 0.6437707986688852\n",
      "epoch 2 batch id 801 loss 1.0208039283752441 train acc 0.6432974406991261\n",
      "epoch 2 batch id 1001 loss 0.7564154863357544 train acc 0.6465409590409591\n",
      "epoch 2 batch id 1201 loss 1.03435480594635 train acc 0.6471950457951707\n",
      "epoch 2 batch id 1401 loss 1.1974906921386719 train acc 0.6499821556031407\n",
      "epoch 2 batch id 1601 loss 0.763526201248169 train acc 0.6520534041224235\n",
      "epoch 2 batch id 1801 loss 1.1741960048675537 train acc 0.6531267351471405\n",
      "epoch 2 batch id 2001 loss 1.0108872652053833 train acc 0.6549693903048476\n",
      "epoch 2 batch id 2201 loss 0.9217188954353333 train acc 0.6574568378009995\n",
      "epoch 2 batch id 2401 loss 0.9743409752845764 train acc 0.6593216368179925\n",
      "epoch 2 batch id 2601 loss 0.9491451382637024 train acc 0.6601667627835448\n",
      "epoch 2 batch id 2801 loss 0.6247503757476807 train acc 0.660991610139236\n",
      "epoch 2 batch id 3001 loss 0.44648292660713196 train acc 0.6629977507497501\n",
      "epoch 2 batch id 3201 loss 0.5298093557357788 train acc 0.6639331458919088\n",
      "epoch 2 train acc 0.6651049337260677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f19faf1e814b30ba9385eb007b93e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6702098376606749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae810db775a8401ea01b891ef4f2a5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.7010499835014343 train acc 0.75\n",
      "epoch 3 batch id 201 loss 0.8250601291656494 train acc 0.6876554726368159\n",
      "epoch 3 batch id 401 loss 0.6361896991729736 train acc 0.6865648379052369\n",
      "epoch 3 batch id 601 loss 0.8381947875022888 train acc 0.6883319467554077\n",
      "epoch 3 batch id 801 loss 0.802291750907898 train acc 0.6884363295880149\n",
      "epoch 3 batch id 1001 loss 0.7579696774482727 train acc 0.6924013486513486\n",
      "epoch 3 batch id 1201 loss 0.8300990462303162 train acc 0.6940570358034971\n",
      "epoch 3 batch id 1401 loss 1.0622456073760986 train acc 0.6969352248394004\n",
      "epoch 3 batch id 1601 loss 0.664162278175354 train acc 0.698567301686446\n",
      "epoch 3 batch id 1801 loss 0.9706401228904724 train acc 0.6986743475846752\n",
      "epoch 3 batch id 2001 loss 0.9214903712272644 train acc 0.7002280109945027\n",
      "epoch 3 batch id 2201 loss 0.7552844882011414 train acc 0.7029759200363471\n",
      "epoch 3 batch id 2401 loss 0.9663523435592651 train acc 0.705109850062474\n",
      "epoch 3 batch id 2601 loss 0.7968223690986633 train acc 0.7063869665513264\n",
      "epoch 3 batch id 2801 loss 0.5518584847450256 train acc 0.7074370760442699\n",
      "epoch 3 batch id 3001 loss 0.3596479296684265 train acc 0.7096280406531156\n",
      "epoch 3 batch id 3201 loss 0.6036629676818848 train acc 0.7107935020306154\n",
      "epoch 3 train acc 0.7123680657830143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155eb96be661429697996f0f853b1bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6709235929738311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a328d11afe4040f988333c60339b750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.6823593378067017 train acc 0.8125\n",
      "epoch 4 batch id 201 loss 0.6303158402442932 train acc 0.744092039800995\n",
      "epoch 4 batch id 401 loss 0.4736688435077667 train acc 0.739713216957606\n",
      "epoch 4 batch id 601 loss 0.7558210492134094 train acc 0.7432404326123128\n",
      "epoch 4 batch id 801 loss 0.6477299332618713 train acc 0.74457709113608\n",
      "epoch 4 batch id 1001 loss 0.5585026144981384 train acc 0.7487824675324676\n",
      "epoch 4 batch id 1201 loss 0.7597712278366089 train acc 0.7510147793505412\n",
      "epoch 4 batch id 1401 loss 0.8614625930786133 train acc 0.7538365453247681\n",
      "epoch 4 batch id 1601 loss 0.4792482554912567 train acc 0.7552115865084322\n",
      "epoch 4 batch id 1801 loss 0.7550599575042725 train acc 0.7556392282065519\n",
      "epoch 4 batch id 2001 loss 0.9364442825317383 train acc 0.7572307596201899\n",
      "epoch 4 batch id 2201 loss 0.5168399214744568 train acc 0.760222626079055\n",
      "epoch 4 batch id 2401 loss 0.7534055113792419 train acc 0.7626379633486048\n",
      "epoch 4 batch id 2601 loss 0.700395405292511 train acc 0.7647659554017685\n",
      "epoch 4 batch id 2801 loss 0.3756881356239319 train acc 0.7662776686897537\n",
      "epoch 4 batch id 3001 loss 0.36057841777801514 train acc 0.7684209430189937\n",
      "epoch 4 batch id 3201 loss 0.2734737992286682 train acc 0.7691639331458919\n",
      "epoch 4 train acc 0.7708026509572902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c0e407afa54287b61bfbae45ac8873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6691936165309571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c5ee72f9b5432b999b34e142d9de6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.4423949718475342 train acc 0.84375\n",
      "epoch 5 batch id 201 loss 0.6938710808753967 train acc 0.8006840796019901\n",
      "epoch 5 batch id 401 loss 0.43662476539611816 train acc 0.7980829177057357\n",
      "epoch 5 batch id 601 loss 0.5720667839050293 train acc 0.8015806988352745\n",
      "epoch 5 batch id 801 loss 0.579055666923523 train acc 0.8043071161048689\n",
      "epoch 5 batch id 1001 loss 0.4663785696029663 train acc 0.8085664335664335\n",
      "epoch 5 batch id 1201 loss 0.5443342924118042 train acc 0.8111469608659451\n",
      "epoch 5 batch id 1401 loss 0.763404905796051 train acc 0.8139944682369736\n",
      "epoch 5 batch id 1601 loss 0.30409371852874756 train acc 0.8155449718925671\n",
      "epoch 5 batch id 1801 loss 0.7939116358757019 train acc 0.8165255413659078\n",
      "epoch 5 batch id 2001 loss 0.7642737627029419 train acc 0.8185126186906547\n",
      "epoch 5 batch id 2201 loss 0.49552828073501587 train acc 0.8207774875056792\n",
      "epoch 5 batch id 2401 loss 0.46359017491340637 train acc 0.8229904206580592\n",
      "epoch 5 batch id 2601 loss 0.6386874318122864 train acc 0.8239979815455594\n",
      "epoch 5 batch id 2801 loss 0.22608791291713715 train acc 0.825341395930025\n",
      "epoch 5 batch id 3001 loss 0.18000930547714233 train acc 0.8272763245584805\n",
      "epoch 5 batch id 3201 loss 0.11923669278621674 train acc 0.8284325210871603\n",
      "epoch 5 train acc 0.8297342906234658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb0d5f6d4af4541bb331ede56bb54f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6763343703589901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a57de0d9cda4a569c1098f75571a930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.3354267477989197 train acc 0.875\n",
      "epoch 6 batch id 201 loss 0.40598466992378235 train acc 0.8532338308457711\n",
      "epoch 6 batch id 401 loss 0.3074420988559723 train acc 0.8516988778054863\n",
      "epoch 6 batch id 601 loss 0.46637555956840515 train acc 0.8545133111480865\n",
      "epoch 6 batch id 801 loss 0.4471226930618286 train acc 0.8554151061173533\n",
      "epoch 6 batch id 1001 loss 0.3513924479484558 train acc 0.8585789210789211\n",
      "epoch 6 batch id 1201 loss 0.3365473747253418 train acc 0.8601165695253955\n",
      "epoch 6 batch id 1401 loss 0.4722123146057129 train acc 0.8622412562455389\n",
      "epoch 6 batch id 1601 loss 0.3823358416557312 train acc 0.8629177076826983\n",
      "epoch 6 batch id 1801 loss 0.6051333546638489 train acc 0.8631836479733481\n",
      "epoch 6 batch id 2001 loss 0.5618377327919006 train acc 0.8644427786106946\n",
      "epoch 6 batch id 2201 loss 0.39600956439971924 train acc 0.8659842117219446\n",
      "epoch 6 batch id 2401 loss 0.24185024201869965 train acc 0.867112661391087\n",
      "epoch 6 batch id 2601 loss 0.5746057033538818 train acc 0.8674067666282199\n",
      "epoch 6 batch id 2801 loss 0.12077949196100235 train acc 0.8671791324526955\n",
      "epoch 6 batch id 3001 loss 0.1695142686367035 train acc 0.8683147284238587\n",
      "epoch 6 batch id 3201 loss 0.08216911554336548 train acc 0.8678928459856294\n",
      "epoch 6 train acc 0.8681363524791359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8344c4533ad42b4ac4c6cd58ff782fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.6754509781328417\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "학습률 e-4, 에포크4) 0.6691936165309571"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 6  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 0.05\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab69142c604e47f1a5c6179b09eedfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.8784935474395752 train acc 0.125\n",
      "epoch 1 batch id 201 loss 1.6341322660446167 train acc 0.2691231343283582\n",
      "epoch 1 batch id 401 loss 1.7473065853118896 train acc 0.2627805486284289\n",
      "epoch 1 batch id 601 loss 1.9483098983764648 train acc 0.2621672212978369\n",
      "epoch 1 batch id 801 loss 1.8602584600448608 train acc 0.25858302122347065\n",
      "epoch 1 batch id 1001 loss 2.070012331008911 train acc 0.2563998501498502\n",
      "epoch 1 batch id 1201 loss 3.2637345790863037 train acc 0.2505203996669442\n",
      "epoch 1 batch id 1401 loss 3.4350531101226807 train acc 0.24386598857958602\n",
      "epoch 1 batch id 1601 loss 2.602599859237671 train acc 0.23826905059337913\n",
      "epoch 1 batch id 1801 loss 13.998085975646973 train acc 0.23349875069405884\n",
      "epoch 1 batch id 2001 loss 13.515995979309082 train acc 0.23024425287356323\n",
      "epoch 1 batch id 2201 loss 10.79781436920166 train acc 0.22743923216719672\n",
      "epoch 1 batch id 2401 loss 9.103242874145508 train acc 0.22573927530195753\n",
      "epoch 1 batch id 2601 loss 13.108366012573242 train acc 0.22362793156478278\n",
      "epoch 1 batch id 2801 loss 9.339090347290039 train acc 0.22094787575865762\n",
      "epoch 1 batch id 3001 loss 11.740947723388672 train acc 0.22002040986337887\n",
      "epoch 1 batch id 3201 loss 6.450082302093506 train acc 0.21865237425804437\n",
      "epoch 1 train acc 0.21792771232204222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694c27b225cb4bd69a189d40a55c994e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.332041852307062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a159b26460a42269a75e9158c4e510c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 11.930437088012695 train acc 0.25\n",
      "epoch 2 batch id 201 loss 8.03327465057373 train acc 0.19760572139303484\n",
      "epoch 2 batch id 401 loss 8.71690845489502 train acc 0.1987998753117207\n",
      "epoch 2 batch id 601 loss 8.592778205871582 train acc 0.19581946755407653\n",
      "epoch 2 batch id 801 loss 11.336365699768066 train acc 0.1970583645443196\n",
      "epoch 2 batch id 1001 loss 5.910654067993164 train acc 0.1981143856143856\n",
      "epoch 2 batch id 1201 loss 10.445281982421875 train acc 0.19764779350541214\n",
      "epoch 2 batch id 1401 loss 6.336135387420654 train acc 0.1982735546038544\n",
      "epoch 2 batch id 1601 loss 10.253129959106445 train acc 0.19897720174890693\n",
      "epoch 2 batch id 1801 loss 7.217033386230469 train acc 0.19784147695724597\n",
      "epoch 2 batch id 2001 loss 7.630019664764404 train acc 0.19730759620189905\n",
      "epoch 2 batch id 2201 loss 7.626842975616455 train acc 0.19699852339845525\n",
      "epoch 2 batch id 2401 loss 7.371237277984619 train acc 0.19741774260724698\n",
      "epoch 2 batch id 2601 loss 9.045645713806152 train acc 0.1969915417147251\n",
      "epoch 2 batch id 2801 loss 7.363991737365723 train acc 0.19683818279186005\n",
      "epoch 2 batch id 3001 loss 5.707592964172363 train acc 0.19690311562812396\n",
      "epoch 2 batch id 3201 loss 10.399080276489258 train acc 0.19761402686660418\n",
      "epoch 2 train acc 0.19760370643102604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9096ee5c89f04ae590a3757af26b93dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.332041852307062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720c17794168433ea4259c867bfefb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 8.392675399780273 train acc 0.34375\n",
      "epoch 3 batch id 201 loss 6.370953559875488 train acc 0.19838308457711443\n",
      "epoch 3 batch id 401 loss 9.300577163696289 train acc 0.1996571072319202\n",
      "epoch 3 batch id 601 loss 6.4138665199279785 train acc 0.20148710482529117\n",
      "epoch 3 batch id 801 loss 8.092765808105469 train acc 0.2016619850187266\n",
      "epoch 3 batch id 1001 loss 5.32517671585083 train acc 0.2024537962037962\n",
      "epoch 3 batch id 1201 loss 10.068965911865234 train acc 0.2016548709408826\n",
      "epoch 3 batch id 1401 loss 9.406394004821777 train acc 0.20110635260528195\n",
      "epoch 3 batch id 1601 loss 5.392395973205566 train acc 0.2005972829481574\n",
      "epoch 3 batch id 1801 loss 6.14544677734375 train acc 0.20037479178234313\n",
      "epoch 3 batch id 2001 loss 7.151872158050537 train acc 0.20039980009995004\n",
      "epoch 3 batch id 2201 loss 5.616616249084473 train acc 0.19989493412085416\n",
      "epoch 3 batch id 2401 loss 3.8167219161987305 train acc 0.20005987088713037\n",
      "epoch 3 batch id 2601 loss 8.663488388061523 train acc 0.20051182237600923\n",
      "epoch 3 batch id 2801 loss 4.855141639709473 train acc 0.1998728132809711\n",
      "epoch 3 batch id 3001 loss 6.256615161895752 train acc 0.20028740419860047\n",
      "epoch 3 batch id 3201 loss 3.5402815341949463 train acc 0.20055256169946892\n",
      "epoch 3 train acc 0.20093888070692195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c65d41f16124d54b24c0ce344befe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.11707027449172941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35199eb2284b44eea3d1f11f66168e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 8.612787246704102 train acc 0.125\n",
      "epoch 4 batch id 201 loss 4.565395832061768 train acc 0.19496268656716417\n",
      "epoch 4 batch id 401 loss 5.748716354370117 train acc 0.19950124688279303\n",
      "epoch 4 batch id 601 loss 5.079095363616943 train acc 0.1975873544093178\n",
      "epoch 4 batch id 801 loss 6.069664001464844 train acc 0.19975031210986266\n",
      "epoch 4 batch id 1001 loss 8.04370403289795 train acc 0.20248501498501498\n",
      "epoch 4 batch id 1201 loss 4.092875957489014 train acc 0.20121253122398003\n",
      "epoch 4 batch id 1401 loss 4.969269752502441 train acc 0.20117326909350464\n",
      "epoch 4 batch id 1601 loss 5.827573776245117 train acc 0.20071439725171766\n",
      "epoch 4 batch id 1801 loss 6.111551284790039 train acc 0.200461549139367\n",
      "epoch 4 batch id 2001 loss 5.749558925628662 train acc 0.20100887056471764\n",
      "epoch 4 batch id 2201 loss 5.1712727546691895 train acc 0.1997103589277601\n",
      "epoch 4 batch id 2401 loss 3.429074287414551 train acc 0.2007887338608913\n",
      "epoch 4 batch id 2601 loss 6.8730621337890625 train acc 0.20128075740099963\n",
      "epoch 4 batch id 2801 loss 3.5212295055389404 train acc 0.20072072474116387\n",
      "epoch 4 batch id 3001 loss 3.3410539627075195 train acc 0.20073517160946353\n",
      "epoch 4 batch id 3201 loss 2.848097801208496 train acc 0.20060137457044674\n",
      "epoch 4 train acc 0.20123036327933236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d9c3f066a7430fa8242f1cd2ef6593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.332041852307062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4449568db8aa49438b287176769cfc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 3.1780529022216797 train acc 0.25\n",
      "epoch 5 batch id 201 loss 3.5592195987701416 train acc 0.20864427860696516\n",
      "epoch 5 batch id 401 loss 3.3476760387420654 train acc 0.20643703241895262\n",
      "epoch 5 batch id 601 loss 3.587517261505127 train acc 0.20564683860232946\n",
      "epoch 5 batch id 801 loss 3.9019222259521484 train acc 0.20657771535580524\n",
      "epoch 5 batch id 1001 loss 2.2047529220581055 train acc 0.2081356143856144\n",
      "epoch 5 batch id 1201 loss 2.800680637359619 train acc 0.20618234804329724\n",
      "epoch 5 batch id 1401 loss 3.0531346797943115 train acc 0.2061250892219843\n",
      "epoch 5 batch id 1601 loss 2.663379669189453 train acc 0.20664818863210493\n",
      "epoch 5 batch id 1801 loss 2.9226787090301514 train acc 0.20693364797334815\n",
      "epoch 5 batch id 2001 loss 2.842444896697998 train acc 0.2070995752123938\n",
      "epoch 5 batch id 2201 loss 2.2258388996124268 train acc 0.20724954566106316\n",
      "epoch 5 batch id 2401 loss 2.091364860534668 train acc 0.2082725947521866\n",
      "epoch 5 batch id 2601 loss 2.7925620079040527 train acc 0.20846549404075357\n",
      "epoch 5 batch id 2801 loss 1.7826306819915771 train acc 0.20903248839700106\n",
      "epoch 5 batch id 3001 loss 2.1606006622314453 train acc 0.2102528323892036\n",
      "epoch 5 batch id 3201 loss 2.338078260421753 train acc 0.2106665885660731\n",
      "epoch 5 train acc 0.21134634266077565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab9d011fdd042d8849c42cb447c0a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.1566244814871716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c204e4b7a4ac4d358a4e8eb81b631ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 2.483792304992676 train acc 0.15625\n",
      "epoch 6 batch id 201 loss 2.037726879119873 train acc 0.23149875621890548\n",
      "epoch 6 batch id 401 loss 2.096045732498169 train acc 0.23145261845386533\n",
      "epoch 6 batch id 601 loss 1.7864364385604858 train acc 0.23247712146422628\n",
      "epoch 6 batch id 801 loss 1.9927068948745728 train acc 0.23431647940074907\n",
      "epoch 6 batch id 1001 loss 1.6718286275863647 train acc 0.23944805194805194\n",
      "epoch 6 batch id 1201 loss 1.7539863586425781 train acc 0.24180370524562864\n",
      "epoch 6 batch id 1401 loss 1.9316866397857666 train acc 0.2434421841541756\n",
      "epoch 6 batch id 1601 loss 1.7280086278915405 train acc 0.24611570893191756\n",
      "epoch 6 batch id 1801 loss 1.8776930570602417 train acc 0.2484730705163798\n",
      "epoch 6 batch id 2001 loss 1.7919366359710693 train acc 0.25181159420289856\n",
      "epoch 6 batch id 2201 loss 1.7567132711410522 train acc 0.2553668786915039\n",
      "epoch 6 batch id 2401 loss 1.6685898303985596 train acc 0.2593450645564348\n",
      "epoch 6 batch id 2601 loss 1.7363793849945068 train acc 0.2622188581314879\n",
      "epoch 6 batch id 2801 loss 1.6237987279891968 train acc 0.2653181899321671\n",
      "epoch 6 batch id 3001 loss 1.7209893465042114 train acc 0.2688166444518494\n",
      "epoch 6 batch id 3201 loss 1.797595739364624 train acc 0.27220985629490785\n",
      "epoch 6 train acc 0.2755031909671085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e32b6a1f7ed4aa58b316542e6e6b0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.332041852307062\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률 0.05 , 에포크\n",
    "(발산)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__학습률5e-5, 에포크3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 3  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9096835d424701ba3b5ea62a784ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.8545067310333252 train acc 0.15625\n",
      "epoch 1 batch id 201 loss 1.527410626411438 train acc 0.28280472636815923\n",
      "epoch 1 batch id 401 loss 1.1838306188583374 train acc 0.3892612219451372\n",
      "epoch 1 batch id 601 loss 1.1383438110351562 train acc 0.45450291181364394\n",
      "epoch 1 batch id 801 loss 1.0952427387237549 train acc 0.48892009987515606\n",
      "epoch 1 batch id 1001 loss 1.0606321096420288 train acc 0.5161713286713286\n",
      "epoch 1 batch id 1201 loss 1.0103175640106201 train acc 0.5336698584512906\n",
      "epoch 1 batch id 1401 loss 1.1667410135269165 train acc 0.5482913990007138\n",
      "epoch 1 batch id 1601 loss 0.9199232459068298 train acc 0.5585961898813242\n",
      "epoch 1 batch id 1801 loss 1.1422905921936035 train acc 0.5668031649083842\n",
      "epoch 1 batch id 2001 loss 1.1484204530715942 train acc 0.574197276361819\n",
      "epoch 1 batch id 2201 loss 0.9804944396018982 train acc 0.5814260563380281\n",
      "epoch 1 batch id 2401 loss 1.1562060117721558 train acc 0.5875026030820492\n",
      "epoch 1 batch id 2601 loss 1.0403692722320557 train acc 0.591575355632449\n",
      "epoch 1 batch id 2801 loss 0.7803412079811096 train acc 0.5957693680828275\n",
      "epoch 1 batch id 3001 loss 0.5735228657722473 train acc 0.6007268410529824\n",
      "epoch 1 batch id 3201 loss 0.5420354008674622 train acc 0.6039226023117775\n",
      "epoch 1 train acc 0.6073607020127639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2744ff4cbf048c5a25ea889239e16f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6740378706406513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cc54e0d9a24002a58199de9d493cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.7684194445610046 train acc 0.6875\n",
      "epoch 2 batch id 201 loss 1.0111333131790161 train acc 0.6694651741293532\n",
      "epoch 2 batch id 401 loss 0.843646764755249 train acc 0.6659912718204489\n",
      "epoch 2 batch id 601 loss 0.9792618155479431 train acc 0.668053244592346\n",
      "epoch 2 batch id 801 loss 0.8164739012718201 train acc 0.6707240948813983\n",
      "epoch 2 batch id 1001 loss 0.7771753072738647 train acc 0.6764173326673326\n",
      "epoch 2 batch id 1201 loss 1.0254918336868286 train acc 0.6795378850957535\n",
      "epoch 2 batch id 1401 loss 1.0714303255081177 train acc 0.6831727337615988\n",
      "epoch 2 batch id 1601 loss 0.7517008185386658 train acc 0.6855090568394753\n",
      "epoch 2 batch id 1801 loss 0.9379293918609619 train acc 0.6876214602998334\n",
      "epoch 2 batch id 2001 loss 0.8575830459594727 train acc 0.6900456021989005\n",
      "epoch 2 batch id 2201 loss 0.8544245362281799 train acc 0.6931224443434802\n",
      "epoch 2 batch id 2401 loss 0.8986386060714722 train acc 0.695192107455227\n",
      "epoch 2 batch id 2601 loss 0.8617163300514221 train acc 0.6969194540561322\n",
      "epoch 2 batch id 2801 loss 0.6166489124298096 train acc 0.6987013566583363\n",
      "epoch 2 batch id 3001 loss 0.4391178488731384 train acc 0.7016515328223926\n",
      "epoch 2 batch id 3201 loss 0.40033411979675293 train acc 0.7033934707903781\n",
      "epoch 2 train acc 0.7052804369170349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038835c022784c9c9d3ec9186f40d856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6905646668715112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68aa81a4d347478e863ede830df9a68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.6888160705566406 train acc 0.71875\n",
      "epoch 3 batch id 201 loss 0.7808974385261536 train acc 0.7492226368159204\n",
      "epoch 3 batch id 401 loss 0.6058851480484009 train acc 0.742752493765586\n",
      "epoch 3 batch id 601 loss 0.7424806952476501 train acc 0.7453202995008319\n",
      "epoch 3 batch id 801 loss 0.7666810750961304 train acc 0.7468398876404494\n",
      "epoch 3 batch id 1001 loss 0.6716791987419128 train acc 0.7514985014985015\n",
      "epoch 3 batch id 1201 loss 0.7556205987930298 train acc 0.7546835970024979\n",
      "epoch 3 batch id 1401 loss 0.6293813586235046 train acc 0.7579853675945754\n",
      "epoch 3 batch id 1601 loss 0.5433908700942993 train acc 0.7600718301061836\n",
      "epoch 3 batch id 1801 loss 0.6680213809013367 train acc 0.7614519711271516\n",
      "epoch 3 batch id 2001 loss 0.7848539352416992 train acc 0.7631028235882059\n",
      "epoch 3 batch id 2201 loss 0.629702091217041 train acc 0.7656746933212176\n",
      "epoch 3 batch id 2401 loss 0.6801443696022034 train acc 0.7678701582673886\n",
      "epoch 3 batch id 2601 loss 0.8584754467010498 train acc 0.7693675509419454\n",
      "epoch 3 batch id 2801 loss 0.4036126434803009 train acc 0.7697139414494824\n",
      "epoch 3 batch id 3001 loss 0.33392953872680664 train acc 0.7714824225258248\n",
      "epoch 3 batch id 3201 loss 0.28766605257987976 train acc 0.7719755545142143\n",
      "epoch 3 train acc 0.7728184830633285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a75066c29c40b0851f013adb9c20a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6939654068725355\n"
     ]
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy = [], []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_accuarcy.append(train_acc / (batch_id+1))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    test_accuarcy.append(test_acc / (batch_id+1))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6073607020127639, 0.7052804369170349, 0.7728184830633285],\n",
       " [0.6740378706406513, 0.6905646668715112, 0.6939654068725355])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuarcy, test_accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model/kobert-v6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 351.79 MB\n"
     ]
    }
   ],
   "source": [
    "# 모델 사이즈 확인\n",
    "import os\n",
    "\n",
    "model_path = 'model/kobert-v6.pt'\n",
    "size = os.path.getsize(model_path) / (1024*1024) # mb 단위\n",
    "print(f\"Model size: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
