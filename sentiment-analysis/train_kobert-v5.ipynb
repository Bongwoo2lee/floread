{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 런타임 애러 방지\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104731</th>\n",
       "      <td>중학교 때 심한 따돌림으로 트라우마가 생겨 사람들과 소통하기가 너무 힘들어.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77275</th>\n",
       "      <td>어, 데리고 나올 거야?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96204</th>\n",
       "      <td>나는 진로는 성적이 충분히 좋게 나오면 결정하기로 했어. 그래도 늦지 않을 것 같아.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38365</th>\n",
       "      <td>아..누나. 좀 일어나보시지. 누나.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46951</th>\n",
       "      <td>안 그러던 동생이 나한테 먹을 거를 갖다 주어서 당황스러웠어.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  emotion\n",
       "104731       중학교 때 심한 따돌림으로 트라우마가 생겨 사람들과 소통하기가 너무 힘들어.        1\n",
       "77275                                     어, 데리고 나올 거야?        5\n",
       "96204   나는 진로는 성적이 충분히 좋게 나오면 결정하기로 했어. 그래도 늦지 않을 것 같아.        1\n",
       "38365                              아..누나. 좀 일어나보시지. 누나.        5\n",
       "46951                안 그러던 동생이 나한테 먹을 거를 갖다 주어서 당황스러웠어.        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v3.csv', index_col=0) \n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64    \n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605ccbd8825b4f43bc36b67f2a171b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.03690242767334 train acc 0.03125\n",
      "epoch 1 batch id 201 loss 1.6281280517578125 train acc 0.22807835820895522\n",
      "epoch 1 batch id 401 loss 1.708240270614624 train acc 0.32917705735660846\n",
      "epoch 1 batch id 601 loss 1.3464723825454712 train acc 0.3913789517470882\n",
      "epoch 1 batch id 801 loss 1.2271583080291748 train acc 0.4364076154806492\n",
      "epoch 1 batch id 1001 loss 1.3684086799621582 train acc 0.4706855644355644\n",
      "epoch 1 batch id 1201 loss 1.232927918434143 train acc 0.49700770191507077\n",
      "epoch 1 batch id 1401 loss 1.0647355318069458 train acc 0.5145431834403997\n",
      "epoch 1 batch id 1601 loss 1.173134684562683 train acc 0.5274242660836976\n",
      "epoch 1 batch id 1801 loss 0.9373440742492676 train acc 0.5393010827318157\n",
      "epoch 1 batch id 2001 loss 1.3755143880844116 train acc 0.5495377311344328\n",
      "epoch 1 batch id 2201 loss 0.8973793983459473 train acc 0.5579992049068605\n",
      "epoch 1 batch id 2401 loss 0.9153470396995544 train acc 0.5650900666389005\n",
      "epoch 1 batch id 2601 loss 0.7026615142822266 train acc 0.5704296424452133\n",
      "epoch 1 batch id 2801 loss 0.7426155209541321 train acc 0.5756091574437701\n",
      "epoch 1 batch id 3001 loss 1.0154318809509277 train acc 0.5807439186937687\n",
      "epoch 1 batch id 3201 loss 0.7877529859542847 train acc 0.5857251640112465\n",
      "epoch 1 train acc 0.5897797005400098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db50ee73dee14e879a89d7d551acc0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6641589209812055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a614bc2d75f4312bd66221bdb54e142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.045026421546936 train acc 0.59375\n",
      "epoch 2 batch id 201 loss 0.8828083872795105 train acc 0.6532960199004975\n",
      "epoch 2 batch id 401 loss 1.096628189086914 train acc 0.6569513715710723\n",
      "epoch 2 batch id 601 loss 0.8334296941757202 train acc 0.658797836938436\n",
      "epoch 2 batch id 801 loss 1.0011390447616577 train acc 0.6602684144818977\n",
      "epoch 2 batch id 1001 loss 1.3065295219421387 train acc 0.6628371628371629\n",
      "epoch 2 batch id 1201 loss 1.1290444135665894 train acc 0.6662156536219817\n",
      "epoch 2 batch id 1401 loss 1.059396743774414 train acc 0.6686295503211992\n",
      "epoch 2 batch id 1601 loss 1.136947751045227 train acc 0.6705184259837601\n",
      "epoch 2 batch id 1801 loss 0.5484654903411865 train acc 0.6734279566907274\n",
      "epoch 2 batch id 2001 loss 1.068651556968689 train acc 0.6752717391304348\n",
      "epoch 2 batch id 2201 loss 0.7853304743766785 train acc 0.6776465243071331\n",
      "epoch 2 batch id 2401 loss 0.6209391355514526 train acc 0.6801853394418992\n",
      "epoch 2 batch id 2601 loss 0.4192003607749939 train acc 0.6818651480199923\n",
      "epoch 2 batch id 2801 loss 0.6916308403015137 train acc 0.6842422349161014\n",
      "epoch 2 batch id 3001 loss 0.7466669082641602 train acc 0.6861566977674108\n",
      "epoch 2 batch id 3201 loss 0.8378002643585205 train acc 0.688349343955014\n",
      "epoch 2 train acc 0.6900803878252332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b35d22f187456681e9c2efe27d3766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6833438956316895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba490d2caf294d339d17ab7d318a363e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.8707109689712524 train acc 0.65625\n",
      "epoch 3 batch id 201 loss 0.7780370712280273 train acc 0.7199937810945274\n",
      "epoch 3 batch id 401 loss 0.827009379863739 train acc 0.7213216957605985\n",
      "epoch 3 batch id 601 loss 0.472354531288147 train acc 0.72363768718802\n",
      "epoch 3 batch id 801 loss 0.6924821734428406 train acc 0.7258504993757803\n",
      "epoch 3 batch id 1001 loss 1.0781960487365723 train acc 0.7279595404595405\n",
      "epoch 3 batch id 1201 loss 0.8106368780136108 train acc 0.7311875520399667\n",
      "epoch 3 batch id 1401 loss 0.8523895144462585 train acc 0.732936295503212\n",
      "epoch 3 batch id 1601 loss 0.8871591091156006 train acc 0.7362195502810743\n",
      "epoch 3 batch id 1801 loss 0.38620689511299133 train acc 0.7392073847862298\n",
      "epoch 3 batch id 2001 loss 0.8478705286979675 train acc 0.742191404297851\n",
      "epoch 3 batch id 2201 loss 0.7078331708908081 train acc 0.7447325079509314\n",
      "epoch 3 batch id 2401 loss 0.5496423244476318 train acc 0.7473578717201166\n",
      "epoch 3 batch id 2601 loss 0.29199060797691345 train acc 0.7492430795847751\n",
      "epoch 3 batch id 2801 loss 0.467692494392395 train acc 0.751561942163513\n",
      "epoch 3 batch id 3001 loss 0.6023640632629395 train acc 0.7533426357880706\n",
      "epoch 3 batch id 3201 loss 0.6842749118804932 train acc 0.7552522649172134\n",
      "epoch 3 train acc 0.7564678448699068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356dabcdcc6648338627bbdcb8845f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6914256542223588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46239afdac084199b8eb1c396828376e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.5455279350280762 train acc 0.90625\n",
      "epoch 4 batch id 201 loss 0.7927102446556091 train acc 0.7874689054726368\n",
      "epoch 4 batch id 401 loss 0.6828895807266235 train acc 0.7853023690773068\n",
      "epoch 4 batch id 601 loss 0.3736403286457062 train acc 0.7867096505823628\n",
      "epoch 4 batch id 801 loss 0.6227283477783203 train acc 0.7883114856429463\n",
      "epoch 4 batch id 1001 loss 0.922927737236023 train acc 0.7919892607392608\n",
      "epoch 4 batch id 1201 loss 0.7746268510818481 train acc 0.7959252706078268\n",
      "epoch 4 batch id 1401 loss 0.5599172115325928 train acc 0.7967969307637401\n",
      "epoch 4 batch id 1601 loss 0.7513710260391235 train acc 0.7996759837601499\n",
      "epoch 4 batch id 1801 loss 0.25761300325393677 train acc 0.8024361465852304\n",
      "epoch 4 batch id 2001 loss 0.5102630257606506 train acc 0.8054254122938531\n",
      "epoch 4 batch id 2201 loss 0.39396217465400696 train acc 0.8078856201726488\n",
      "epoch 4 batch id 2401 loss 0.3194902539253235 train acc 0.8105737192836319\n",
      "epoch 4 batch id 2601 loss 0.2435542494058609 train acc 0.8122837370242214\n",
      "epoch 4 batch id 2801 loss 0.5056903958320618 train acc 0.813972688325598\n",
      "epoch 4 batch id 3001 loss 0.3996787667274475 train acc 0.8154365211596135\n",
      "epoch 4 batch id 3201 loss 0.460267037153244 train acc 0.8166490940331147\n",
      "epoch 4 train acc 0.817369293078056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c987a5902c404fbf7f7f50e212a1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6836223562247146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca9d479c22d488aaffcae4c43077132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.46702736616134644 train acc 0.8125\n",
      "epoch 5 batch id 201 loss 0.6349192261695862 train acc 0.8361318407960199\n",
      "epoch 5 batch id 401 loss 0.47349485754966736 train acc 0.8362687032418953\n",
      "epoch 5 batch id 601 loss 0.4137037992477417 train acc 0.8374064059900166\n",
      "epoch 5 batch id 801 loss 0.5722661018371582 train acc 0.839185393258427\n",
      "epoch 5 batch id 1001 loss 0.7273741364479065 train acc 0.8425012487512488\n",
      "epoch 5 batch id 1201 loss 0.44373244047164917 train acc 0.8457014987510408\n",
      "epoch 5 batch id 1401 loss 0.35330796241760254 train acc 0.8458690221270521\n",
      "epoch 5 batch id 1601 loss 0.47852641344070435 train acc 0.847556214865709\n",
      "epoch 5 batch id 1801 loss 0.17743375897407532 train acc 0.8486604664075513\n",
      "epoch 5 batch id 2001 loss 0.5358647704124451 train acc 0.8506059470264867\n",
      "epoch 5 batch id 2201 loss 0.30664321780204773 train acc 0.851899704679691\n",
      "epoch 5 batch id 2401 loss 0.29323744773864746 train acc 0.8530690337359433\n",
      "epoch 5 batch id 2601 loss 0.12555105984210968 train acc 0.8533016147635525\n",
      "epoch 5 batch id 2801 loss 0.350952684879303 train acc 0.8542819528739736\n",
      "epoch 5 batch id 3001 loss 0.2907114624977112 train acc 0.8549233588803732\n",
      "epoch 5 batch id 3201 loss 0.39489150047302246 train acc 0.8553186504217432\n",
      "epoch 5 train acc 0.8550441826215022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6060498652f64ee0a86299611b35ffc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6853523326675884\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model/kobert-v5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 351.79 MB\n"
     ]
    }
   ],
   "source": [
    "# 모델 사이즈 확인(파라미터 동일)\n",
    "import os\n",
    "\n",
    "model_path = 'model/kobert-v5.pt'\n",
    "size = os.path.getsize(model_path) / (1024*1024) # mb 단위\n",
    "print(f\"Model size: {size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
