{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114811</th>\n",
       "      <td>옛날 재능만 믿고 게으름 피다 퇴물 되놓고... 어디다 탓을 해?!!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80840</th>\n",
       "      <td>꿈이 현실로</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44819</th>\n",
       "      <td>나 지원이랑 친구 문제로 싸웠어. 요새 맨날 싸워서 지친다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43240</th>\n",
       "      <td>학교에 가는 게 너무 무서워. 친구들이 또 괴롭히러 올 것 같아.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46105</th>\n",
       "      <td>오늘도 여자친구와 싸웠어. 그래서 너무 기분이 안 좋아.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentence  emotion\n",
       "114811  옛날 재능만 믿고 게으름 피다 퇴물 되놓고... 어디다 탓을 해?!!        4\n",
       "80840                                   꿈이 현실로        0\n",
       "44819        나 지원이랑 친구 문제로 싸웠어. 요새 맨날 싸워서 지친다.        1\n",
       "43240     학교에 가는 게 너무 무서워. 친구들이 또 괴롭히러 올 것 같아.        1\n",
       "46105          오늘도 여자친구와 싸웠어. 그래서 너무 기분이 안 좋아.        1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv('data/병합데이터셋-v2.csv', index_col=0) \n",
    "\n",
    "# 감정을 정수 라벨로 변경\n",
    "emotions = {'행복': 0, '불안': 1, '놀람': 2, '슬픔': 3, '분노': 4, '중립': 5 }\n",
    "train_set['emotion'] = train_set.emotion.map(emotions)\n",
    "\n",
    "train_set.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gluonnlp as nlp\n",
    "\n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# Setting parameters(KoBERT finetuning 베에스 라인) -> \n",
    "max_len = 64    #베이스라인 64\n",
    "batch_size = 32 #베이스라인 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 사용되는 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,     \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n",
    "train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n",
    "\n",
    "# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n",
    "train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "# 데이터셋을 Bert모델에 입력할 수 있게 변환\n",
    "train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n",
    "\n",
    "# 배치데이터셋 생성\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "# 예측 반환\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d99b964a20247348ed27ab089046c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.1148581504821777 train acc 0.125\n",
      "epoch 1 batch id 201 loss 1.7056244611740112 train acc 0.23491915422885573\n",
      "epoch 1 batch id 401 loss 1.4151387214660645 train acc 0.3218516209476309\n",
      "epoch 1 batch id 601 loss 1.277246356010437 train acc 0.3834234608985025\n",
      "epoch 1 batch id 801 loss 1.174025297164917 train acc 0.43184300873907616\n",
      "epoch 1 batch id 1001 loss 1.0892586708068848 train acc 0.4698114385614386\n",
      "epoch 1 batch id 1201 loss 1.213165283203125 train acc 0.49594088259783514\n",
      "epoch 1 batch id 1401 loss 0.9456455707550049 train acc 0.5145208779443254\n",
      "epoch 1 batch id 1601 loss 1.2842438220977783 train acc 0.5286930043722673\n",
      "epoch 1 batch id 1801 loss 1.0243161916732788 train acc 0.540047196002221\n",
      "epoch 1 batch id 2001 loss 0.9356043338775635 train acc 0.5483195902048975\n",
      "epoch 1 batch id 2201 loss 0.6708167195320129 train acc 0.5568491594729669\n",
      "epoch 1 batch id 2401 loss 0.711145281791687 train acc 0.5647646813827571\n",
      "epoch 1 batch id 2601 loss 0.8457648754119873 train acc 0.570850153787005\n",
      "epoch 1 batch id 2801 loss 0.8385752439498901 train acc 0.57704837558015\n",
      "epoch 1 batch id 3001 loss 0.6655545830726624 train acc 0.581264578473842\n",
      "epoch 1 batch id 3201 loss 0.8119400143623352 train acc 0.5855006248047485\n",
      "epoch 1 batch id 3401 loss 0.8832395672798157 train acc 0.5890455013231403\n",
      "epoch 1 train acc 0.5893255094503804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554ca30f3fab4f359090666d11267f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6717282863849765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710b4e3a2d614d0aa9bcab393df43a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.7657519578933716 train acc 0.6875\n",
      "epoch 2 batch id 201 loss 0.9162696003913879 train acc 0.6623134328358209\n",
      "epoch 2 batch id 401 loss 0.7363892197608948 train acc 0.660458229426434\n",
      "epoch 2 batch id 601 loss 0.8943337798118591 train acc 0.6624376039933444\n",
      "epoch 2 batch id 801 loss 0.8812430500984192 train acc 0.6655742821473158\n",
      "epoch 2 batch id 1001 loss 1.0940327644348145 train acc 0.6683004495504495\n",
      "epoch 2 batch id 1201 loss 0.9513697624206543 train acc 0.6724604496253123\n",
      "epoch 2 batch id 1401 loss 0.8898323178291321 train acc 0.6756334760885082\n",
      "epoch 2 batch id 1601 loss 1.1755784749984741 train acc 0.678384603372892\n",
      "epoch 2 batch id 1801 loss 0.7788414359092712 train acc 0.6802991393670184\n",
      "epoch 2 batch id 2001 loss 0.7936946153640747 train acc 0.6817684907546226\n",
      "epoch 2 batch id 2201 loss 0.6076501607894897 train acc 0.6839930713312131\n",
      "epoch 2 batch id 2401 loss 0.6516362428665161 train acc 0.6864978134110787\n",
      "epoch 2 batch id 2601 loss 0.7371137738227844 train acc 0.6878724529027297\n",
      "epoch 2 batch id 2801 loss 0.8192486763000488 train acc 0.6901441449482327\n",
      "epoch 2 batch id 3001 loss 0.6330365538597107 train acc 0.6912695768077307\n",
      "epoch 2 batch id 3201 loss 0.7287194728851318 train acc 0.6927717900656045\n",
      "epoch 2 batch id 3401 loss 0.7357558012008667 train acc 0.6939686856806822\n",
      "epoch 2 train acc 0.6941716751659381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d263d72dce264981bb27dcaeeeb5a7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6839055164319249\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ef2c5da2f74505a8c3b8604b6aaed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.6667922735214233 train acc 0.6875\n",
      "epoch 3 batch id 201 loss 0.7204795479774475 train acc 0.7288557213930348\n",
      "epoch 3 batch id 401 loss 0.5676817297935486 train acc 0.7220230673316709\n",
      "epoch 3 batch id 601 loss 0.6431615948677063 train acc 0.7214538269550749\n",
      "epoch 3 batch id 801 loss 0.7104094624519348 train acc 0.7249141697877652\n",
      "epoch 3 batch id 1001 loss 1.2185546159744263 train acc 0.7277722277722277\n",
      "epoch 3 batch id 1201 loss 0.8469130992889404 train acc 0.7325145711906744\n",
      "epoch 3 batch id 1401 loss 0.6633368134498596 train acc 0.7362821199143469\n",
      "epoch 3 batch id 1601 loss 0.8722359538078308 train acc 0.7391083697688945\n",
      "epoch 3 batch id 1801 loss 0.5395464897155762 train acc 0.7422612437534702\n",
      "epoch 3 batch id 2001 loss 0.40944766998291016 train acc 0.7445183658170914\n",
      "epoch 3 batch id 2201 loss 0.4978044629096985 train acc 0.7473875511131304\n",
      "epoch 3 batch id 2401 loss 0.6277294158935547 train acc 0.7505987088713036\n",
      "epoch 3 batch id 2601 loss 0.6273060441017151 train acc 0.7522106881968473\n",
      "epoch 3 batch id 2801 loss 0.6510944366455078 train acc 0.7543734380578365\n",
      "epoch 3 batch id 3001 loss 0.5850094556808472 train acc 0.7553003165611463\n",
      "epoch 3 batch id 3201 loss 0.5703198909759521 train acc 0.7567752264917214\n",
      "epoch 3 batch id 3401 loss 0.6319071650505066 train acc 0.7578653337253749\n",
      "epoch 3 train acc 0.7580774698478226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9616ad33f642451b90e5b4b80ffbd6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6806778169014085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cce54a5b0d742f89d5960807ac286d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.6173720955848694 train acc 0.71875\n",
      "epoch 4 batch id 201 loss 0.6561905145645142 train acc 0.7888681592039801\n",
      "epoch 4 batch id 401 loss 0.4376693069934845 train acc 0.7874064837905237\n",
      "epoch 4 batch id 601 loss 0.6127034425735474 train acc 0.7888415141430949\n",
      "epoch 4 batch id 801 loss 0.6352947354316711 train acc 0.7931101747815231\n",
      "epoch 4 batch id 1001 loss 0.7727664709091187 train acc 0.794080919080919\n",
      "epoch 4 batch id 1201 loss 0.5640541315078735 train acc 0.7971221898417985\n",
      "epoch 4 batch id 1401 loss 0.3727705776691437 train acc 0.7999643112062812\n",
      "epoch 4 batch id 1601 loss 0.8904934525489807 train acc 0.803853060587133\n",
      "epoch 4 batch id 1801 loss 0.3728691637516022 train acc 0.8058717379233759\n",
      "epoch 4 batch id 2001 loss 0.2695982754230499 train acc 0.8085488505747126\n",
      "epoch 4 batch id 2201 loss 0.4039473831653595 train acc 0.8115771240345298\n",
      "epoch 4 batch id 2401 loss 0.36119940876960754 train acc 0.8143612036651395\n",
      "epoch 4 batch id 2601 loss 0.4646156430244446 train acc 0.8158040176855056\n",
      "epoch 4 batch id 2801 loss 0.5680519342422485 train acc 0.8175205283827205\n",
      "epoch 4 batch id 3001 loss 0.34859591722488403 train acc 0.8182376707764079\n",
      "epoch 4 batch id 3201 loss 0.4736439883708954 train acc 0.8186504217432052\n",
      "epoch 4 batch id 3401 loss 0.46399709582328796 train acc 0.8197680829167892\n",
      "epoch 4 train acc 0.8199896668083212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b2bccf057b4e358d636fbe21555469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6826951291079812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecb4938c9814c508d33cbdec15fca81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.3979465067386627 train acc 0.875\n",
      "epoch 5 batch id 201 loss 0.38970738649368286 train acc 0.8390858208955224\n",
      "epoch 5 batch id 401 loss 0.366070032119751 train acc 0.8396197007481296\n",
      "epoch 5 batch id 601 loss 0.575049638748169 train acc 0.8407861896838602\n",
      "epoch 5 batch id 801 loss 0.49514150619506836 train acc 0.8416432584269663\n",
      "epoch 5 batch id 1001 loss 0.6481998562812805 train acc 0.842313936063936\n",
      "epoch 5 batch id 1201 loss 0.4193129539489746 train acc 0.8452851790174855\n",
      "epoch 5 batch id 1401 loss 0.34907081723213196 train acc 0.8468058529621699\n",
      "epoch 5 batch id 1601 loss 0.7552070617675781 train acc 0.8493714865708932\n",
      "epoch 5 batch id 1801 loss 0.31389153003692627 train acc 0.8509682121043864\n",
      "epoch 5 batch id 2001 loss 0.14670021831989288 train acc 0.8529953773113443\n",
      "epoch 5 batch id 2201 loss 0.38508206605911255 train acc 0.8549238982280781\n",
      "epoch 5 batch id 2401 loss 0.23026664555072784 train acc 0.8567003331945023\n",
      "epoch 5 batch id 2601 loss 0.36578166484832764 train acc 0.8572544213763937\n",
      "epoch 5 batch id 2801 loss 0.6295673251152039 train acc 0.858153338093538\n",
      "epoch 5 batch id 3001 loss 0.35847723484039307 train acc 0.8580160779740087\n",
      "epoch 5 batch id 3201 loss 0.45423778891563416 train acc 0.8578569197125898\n",
      "epoch 5 batch id 3401 loss 0.3502810597419739 train acc 0.8580380770361659\n",
      "epoch 5 train acc 0.8581627155172413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f026d53219949f996d1aeeaa58a8ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6814113849765259\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n",
    "\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model/kobert-v4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 351.79 MB\n"
     ]
    }
   ],
   "source": [
    "# 모델 사이즈 확인(파라미터 동일)\n",
    "import os\n",
    "\n",
    "model_path = 'model/kobert-v4.pt'\n",
    "size = os.path.getsize(model_path) / (1024*1024) # mb 단위\n",
    "print(f\"Model size: {size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
