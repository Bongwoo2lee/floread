{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>잇수로 치면 여기서 거기가 시오 리가 넘는답니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>만일 김첨지가주기를 띠지 않았던들 한 발을 대문에 들여놓았을 제 그곳을 지배하는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>“갔다가 그저 오기가 안됐데그려.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>김첨지는 구걸하는 거지나 무엇같이 연해연방 그의 기색을 살피며,  “아씨, 정거장...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>김첨지는 화증을 내며 확신 있게 소리를 질렀으되 그 소리엔 안 죽은 것을믿으려고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>“아따 이놈아, 사십 전이 그리 끔찍하냐.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>아마도 그 학교 기숙사에 있는 이로 동기방학을 이용하여 귀향하려 함이리라.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>“아따, 재미 안 좋다고 술 못 먹을 낸가.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 book1\n",
       "61                         잇수로 치면 여기서 거기가 시오 리가 넘는답니다.\n",
       "222   만일 김첨지가주기를 띠지 않았던들 한 발을 대문에 들여놓았을 제 그곳을 지배하는 ...\n",
       "187                                 “갔다가 그저 오기가 안됐데그려.\n",
       "106   김첨지는 구걸하는 거지나 무엇같이 연해연방 그의 기색을 살피며,  “아씨, 정거장...\n",
       "217   김첨지는 화증을 내며 확신 있게 소리를 질렀으되 그 소리엔 안 죽은 것을믿으려고 ...\n",
       "158                            “아따 이놈아, 사십 전이 그리 끔찍하냐.\n",
       "33           아마도 그 학교 기숙사에 있는 이로 동기방학을 이용하여 귀향하려 함이리라.\n",
       "143                           “아따, 재미 안 좋다고 술 못 먹을 낸가."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample1 = pd.read_csv('data/output.csv', names=[\"book1\"])  #index_col=0 하면 안됨\n",
    "sample1.sample(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: 지정된 모듈을 찾을 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\matplotlib\\__init__.py:109\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    107\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[0;32m    110\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[0;32m    111\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m mplDeprecation  \u001b[39m# deprecated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfontconfig_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\matplotlib\\colors.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPngImagePlugin\u001b[39;00m \u001b[39mimport\u001b[39;00m PngInfo\n\u001b[0;32m     54\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KangIW\\anaconda3\\envs\\kobert0\\lib\\site-packages\\PIL\\Image.py:100\u001b[0m\n\u001b[0;32m     91\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: 지정된 모듈을 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book1'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "#kobert\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# GPU 사용시 필요\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "tok = tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6, #클래스 수 조정\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('model/kobert-v3.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  # 에포크 횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
    "                 pad, pair):\n",
    "   \n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emos = ['행복','불안','놀람', '슬픔','분노','중립']\n",
    "def sa_demo(sentence):\n",
    "    res = predict(sentence)\n",
    "    print(\"문장에서 \"+ emos[res] + \"이(가) 느껴집니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['행복',\n",
       " '중립',\n",
       " '행복',\n",
       " '중립',\n",
       " '중립',\n",
       " '행복',\n",
       " '행복',\n",
       " '행복',\n",
       " '분노',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '불안',\n",
       " '불안',\n",
       " '분노',\n",
       " '분노',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '분노',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '불안',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '불안',\n",
       " '놀람',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '불안',\n",
       " '놀람',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '행복',\n",
       " '중립',\n",
       " '분노',\n",
       " '놀람',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '분노',\n",
       " '중립',\n",
       " '불안',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '행복',\n",
       " '슬픔',\n",
       " '행복',\n",
       " '행복',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '분노',\n",
       " '행복',\n",
       " '불안',\n",
       " '중립',\n",
       " '불안',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '불안',\n",
       " '불안',\n",
       " '중립',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '행복',\n",
       " '행복',\n",
       " '행복',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '행복',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '행복',\n",
       " '중립',\n",
       " '행복',\n",
       " '중립',\n",
       " '중립',\n",
       " '불안',\n",
       " '중립',\n",
       " '행복',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '분노',\n",
       " '행복',\n",
       " '행복',\n",
       " '분노',\n",
       " '놀람',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '슬픔',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '분노',\n",
       " '중립',\n",
       " '놀람',\n",
       " '중립',\n",
       " '불안',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '놀람',\n",
       " '분노',\n",
       " '불안',\n",
       " '분노',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '불안',\n",
       " '불안',\n",
       " '놀람',\n",
       " '분노',\n",
       " '분노',\n",
       " '분노',\n",
       " '슬픔',\n",
       " '슬픔',\n",
       " '슬픔',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '놀람',\n",
       " '슬픔',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '중립',\n",
       " '분노',\n",
       " '중립',\n",
       " '분노',\n",
       " '슬픔',\n",
       " '분노',\n",
       " '중립']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emos = ('행복','불안','놀람', '슬픔','분노','중립')\n",
    "res = []\n",
    "for index, data in sample1.iterrows():\n",
    "    res.append(emos[predict(data['book1'])])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model/kobert-v3.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    dataset = [[sentence, '0']]\n",
    "    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    model.eval()\n",
    "    answer = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        for logits in out:\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            answer = np.argmax(logits)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'행복': 19, '불안': 15, '놀람': 15, '슬픔': 26, '분노': 48, '중립': 133}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emos = ('행복','불안','놀람', '슬픔','분노','중립')\n",
    "res = {'행복':0,'불안':0,'놀람':0, '슬픔':0,'분노':0,'중립':0}\n",
    "\n",
    "for index, data in sample1.iterrows():\n",
    "    res[emos[predict(data['book1'])]] += 1\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행복: 19\n",
      "중립: 133\n",
      "분노: 48\n",
      "슬픔: 26\n",
      "불안: 15\n",
      "놀람: 15\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(res)\n",
    "\n",
    "\n",
    "for emotion, count in counter.items():\n",
    "    print(f\"{emotion}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# 딕셔너리 생성\u001b[39;00m\n\u001b[0;32m      4\u001b[0m my_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m행복\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m중립\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m분노\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m7\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m슬픔\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m불안\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 딕셔너리 생성\n",
    "my_dict = {\"행복\": 5, \"중립\": 10, \"분노\": 7, \"슬픔\": 3, \"불안\": 2}\n",
    "\n",
    "# 키와 값 리스트 추출\n",
    "labels = list(my_dict.keys())\n",
    "sizes = list(my_dict.values())\n",
    "\n",
    "# 원형 그래프 그리기\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "ax1.axis('equal')  # 원형을 유지하도록 축 조정\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
