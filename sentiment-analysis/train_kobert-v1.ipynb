{"cells":[{"cell_type":"markdown","metadata":{"id":"S3-WDCjRgRQV"},"source":["## **데이터 불러오기**"]},{"cell_type":"markdown","metadata":{},"source":["코랩"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25870,"status":"ok","timestamp":1682772020161,"user":{"displayName":"강인우","userId":"07996460503432484159"},"user_tz":-540},"id":"HpPgwwIF0_SF","outputId":"389e4d44-8a2e-466e-d143-278fa841a1bb"},"outputs":[],"source":["# from google.colab import drive  #코랩\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772,"status":"ok","timestamp":1682733296033,"user":{"displayName":"강인우","userId":"07996460503432484159"},"user_tz":-540},"id":"RR7DEAZDygS2","outputId":"f42421b3-3b43-40f7-f2db-06c1bd405628"},"outputs":[],"source":["# 밑에서 상대경로 안되면 sentiment-analysis 까지 이동\n","# %cd /content/drive/MyDrive/Github/floread/sentiment-analysis  #코랩\n","%cd sentiment-analysis"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":757,"status":"ok","timestamp":1682729153549,"user":{"displayName":"강인우","userId":"07996460503432484159"},"user_tz":-540},"id":"Ygr3LSsJgRQY","outputId":"ee6a3099-bfda-43e8-bf59-c1d6234d6422"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>42271</th>\n","      <td>맞벌이하는데 아이를 유치원에 데려다주고 다시 집으로 데리고 오는 게 쉽지가 않아.</td>\n","      <td>불안</td>\n","    </tr>\n","    <tr>\n","      <th>1093</th>\n","      <td>나는 내 주위 사람들 중에 내가 제일 노래를 잘한다고 생각해.</td>\n","      <td>기쁨</td>\n","    </tr>\n","    <tr>\n","      <th>4792</th>\n","      <td>이번에 청약이 또 떨어졌어.</td>\n","      <td>슬픔</td>\n","    </tr>\n","    <tr>\n","      <th>7408</th>\n","      <td>부서원 사람들과 대화가 잘 안 되는 것 같아서 걱정이야.</td>\n","      <td>상처</td>\n","    </tr>\n","    <tr>\n","      <th>49823</th>\n","      <td>사고로 얼굴에 흉이 진 후로 밖으로 나가질 못하겠어.</td>\n","      <td>슬픔</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence emotion\n","42271  맞벌이하는데 아이를 유치원에 데려다주고 다시 집으로 데리고 오는 게 쉽지가 않아.      불안\n","1093              나는 내 주위 사람들 중에 내가 제일 노래를 잘한다고 생각해.      기쁨\n","4792                                 이번에 청약이 또 떨어졌어.      슬픔\n","7408                 부서원 사람들과 대화가 잘 안 되는 것 같아서 걱정이야.      상처\n","49823                  사고로 얼굴에 흉이 진 후로 밖으로 나가질 못하겠어.      슬픔"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","train_set = pd.read_csv('data/감성대화말뭉치(병합).csv', index_col=0) #첫 번째 열이 인덱스 열일 때 Unnamed: 0 빼기\n","train_set.sample(n=5)"]},{"cell_type":"markdown","metadata":{"id":"BX3e1bR-gRQa"},"source":["**감정을 정수 라벨로 변경**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682729157271,"user":{"displayName":"강인우","userId":"07996460503432484159"},"user_tz":-540},"id":"hu27JzDZgRQa","outputId":"b09d1cc7-67ac-4b99-cf2f-f3d2e895a5f5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3479</th>\n","      <td>내 잘못도 아닌데 정직 처분을 받아 너무 속상해.</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>43386</th>\n","      <td>아내와 사별한 이후 아들과 어떤 교류도 없이 지내고 있어. 내가 왜 사는 건지 모르겠어.</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>34020</th>\n","      <td>친구들한테 무시당하는 게 일상이 되어버려서 서러워. 오늘도 말을 걸었는데 무시당했어.</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>19368</th>\n","      <td>난 알코올 중독을 앓고 있어. 그런데도 친구들은 나에게 술을 권해서 이따금 화가 나.</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>8272</th>\n","      <td>내가 원하던 일은 이게 아닌데.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                sentence  emotion\n","3479                         내 잘못도 아닌데 정직 처분을 받아 너무 속상해.        5\n","43386  아내와 사별한 이후 아들과 어떤 교류도 없이 지내고 있어. 내가 왜 사는 건지 모르겠어.        5\n","34020    친구들한테 무시당하는 게 일상이 되어버려서 서러워. 오늘도 말을 걸었는데 무시당했어.        4\n","19368    난 알코올 중독을 앓고 있어. 그런데도 친구들은 나에게 술을 권해서 이따금 화가 나.        4\n","8272                                   내가 원하던 일은 이게 아닌데.        1"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# 감정을 정수 라벨로 변경\n","emotions = {'기쁨': 0, '불안': 1, '당황': 2, '슬픔': 3, '분노': 4, '상처': 5}\n","train_set['emotion'] = train_set.emotion.map(emotions)\n","\n","train_set.sample(n=5)"]},{"cell_type":"markdown","metadata":{"id":"35ot3r9-gRQb"},"source":["__패키지 설치: Korean BERT pre-trained cased (KoBERT) for Huggingface Transformers__\n","https://github.com/SKTBrain/KoBERT/blob/master/kobert_hf/requirements.txt\n"]},{"cell_type":"markdown","metadata":{},"source":["-> 가상환경 설정 `ve_kobert.ipynb`참고"]},{"cell_type":"markdown","metadata":{},"source":["(in colab)"]},{"cell_type":"markdown","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":277388,"status":"ok","timestamp":1682754226080,"user":{"displayName":"강인우","userId":"07996460503432484159"},"user_tz":-540},"id":"ed6S9ZcNgRQb","outputId":"f123943a-bc02-45c0-acf8-bee7d90dc106"},"source":["```python\n","%pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n","%pip install tqdm   \n","%pip install sentencepiece transformers\n","%pip install torch\n","\n","%pip install mxnet\n","# !pip install -U --pre \"mxnet-cu118>=2.0.0a\"   # cu118이랑 맞는 버전 없음\n","# !pip install -U --pre \"mxnet>=2.0.0a\"   #cpu only\n","\n","%pip install gluonnlp\n","```"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"elapsed":5153,"status":"error","timestamp":1682733114791,"user":{"displayName":"강인우","userId":"07996460503432484159"},"user_tz":-540},"id":"oOIOUfhxgRQb","outputId":"beca1699-b3db-4610-d690-32fbc4cb34ab"},"outputs":[],"source":["# 라이브러리 불러오기\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","# from tqdm import tqdm, tqdm_notebook  //\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split\n","import gluonnlp as nlp\n","\n","#kobert\n","from kobert_tokenizer import KoBERTTokenizer\n","\n","# transformers\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","# GPU 사용시 필요\n","#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(\"cuda\")\n","#device = torch.device('cpu')"]},{"cell_type":"markdown","metadata":{"id":"YJzDTgp-gRQb"},"source":["**토크나이저, pretrained 모델, vocabulary 로드**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"oCxicLh1gRQb","outputId":"8493137f-f5ea-4cf1-fcf8-617923b5cad7"},"outputs":[],"source":["tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n","tok = tokenizer.tokenize\n","\n","# Setting parameters(KoBERT finetuning 베에스 라인) -> \n","max_len = 32    #베이스라인 64\n","batch_size = 32 #베이스라인 64\n","warmup_ratio = 0.1\n","num_epochs = 5  # 에포크 횟수\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"]},{"cell_type":"markdown","metadata":{"id":"2LSxGMWtgRQc"},"source":["**KOBert 클래스 정의**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"382KXXNDgRQc"},"outputs":[],"source":["# 모델에 사용되는 데이터셋 클래스 정의\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n","                 pad, pair):\n","   \n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","        \n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","         \n","    def __len__(self):\n","        return (len(self.labels))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"aZLeXNcQgRQc"},"outputs":[],"source":["# 감성 분류 모델 정의\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=6, \n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"]},{"cell_type":"markdown","metadata":{"id":"JVOUPnZfgRQd"},"source":["**데이터 셋 피팅**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CCQCBs6PgRQd"},"outputs":[],"source":["# 모델 학습에 사용할 데이터셋을 [data, label] 배열로 피팅\n","train_set_data = [[i, str(j)] for i, j in zip(train_set['sentence'], train_set['emotion'])]\n","\n","# sklearn 의 train_test_split 모듈-> 4:1로 학습&검증 데이터를 분류 \n","train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)\n","\n","# 데이터셋을 Bert모델에 입력할 수 있게 변환\n","train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)\n","test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)\n","\n","# 배치데이터셋 생성\n","train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=0)    # num_workers: 데이터 로딩할때 쓰는 프로세스 수(로딩속도)\n","test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=0)"]},{"cell_type":"markdown","metadata":{},"source":["모델 선언"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# 모델 선언\n","model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"]},{"cell_type":"markdown","metadata":{"id":"oivU-srigRQc"},"source":["**정확도 계산 함수**"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"FuY9qdjwgRQc"},"outputs":[],"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"6X-hZ-T_gRQc"},"outputs":[],"source":["# 입력에 대한 예측 반환\n","def predict(sentence):\n","    dataset = [[sentence, '0']]\n","    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=0)  #로컬에서는 디폴트(0)으로 수정\n","    model.eval()\n","    answer = 0\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        for logits in out:\n","            logits = logits.detach().cpu().numpy()\n","            answer = np.argmax(logits)\n","    return answer"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU 코어 수: 6\n","사용 가능한 메모리: 9935974400\n"]}],"source":["import os\n","import psutil\n","\n","cpu_count = os.cpu_count()\n","available_memory = psutil.virtual_memory().available\n","\n","print(\"CPU 코어 수:\", cpu_count)\n","print(\"사용 가능한 메모리:\", available_memory)"]},{"cell_type":"markdown","metadata":{},"source":["로컬에서 RuntimeError방지"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gc\n","import torch\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"DsvYsri9gRQd"},"source":["학습"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Nb4e6lWSgRQd"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6acdd4bb08b14d17a57ed507250a247e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1457 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 1 batch id 1 loss 1.7916381359100342 train acc 0.125\n","epoch 1 batch id 201 loss 1.858939528465271 train acc 0.1982276119402985\n","epoch 1 batch id 401 loss 1.2915337085723877 train acc 0.30860349127182046\n","epoch 1 batch id 601 loss 1.2887184619903564 train acc 0.37583194675540765\n","epoch 1 batch id 801 loss 1.2145276069641113 train acc 0.4165106117353308\n","epoch 1 batch id 1001 loss 1.274234652519226 train acc 0.4406843156843157\n","epoch 1 batch id 1201 loss 1.4031423330307007 train acc 0.45691090757701913\n","epoch 1 batch id 1401 loss 0.9159780740737915 train acc 0.4697760528194147\n","epoch 1 train acc 0.47312542896362386\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c50a8494dd84ef68c7811908226e5bb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/365 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 1 test acc 0.5810053816046966\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57e0b14221594b149c89c835478206b4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1457 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 2 batch id 1 loss 1.172863245010376 train acc 0.5\n","epoch 2 batch id 201 loss 1.3949519395828247 train acc 0.5558146766169154\n","epoch 2 batch id 401 loss 0.9046579003334045 train acc 0.5660847880299252\n","epoch 2 batch id 601 loss 1.2276753187179565 train acc 0.5680636439267887\n","epoch 2 batch id 801 loss 1.1491986513137817 train acc 0.5765059300873908\n","epoch 2 batch id 1001 loss 1.0736383199691772 train acc 0.5805444555444556\n","epoch 2 batch id 1201 loss 1.3014317750930786 train acc 0.5839664862614488\n","epoch 2 batch id 1401 loss 0.7532556056976318 train acc 0.5865899357601713\n","epoch 2 train acc 0.588244966826813\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1fff2a66f8e4390824afd7f771b0c45","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/365 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 2 test acc 0.5921844422700586\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8b93b0058284f358e143be7c1c31a31","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1457 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 3 batch id 1 loss 1.026536464691162 train acc 0.65625\n","epoch 3 batch id 201 loss 1.434572696685791 train acc 0.6005907960199005\n","epoch 3 batch id 401 loss 0.8356627225875854 train acc 0.6161159600997507\n","epoch 3 batch id 601 loss 1.1390113830566406 train acc 0.6206322795341098\n","epoch 3 batch id 801 loss 1.0710867643356323 train acc 0.6321395131086143\n","epoch 3 batch id 1001 loss 1.0520281791687012 train acc 0.6391733266733267\n","epoch 3 batch id 1201 loss 1.225669026374817 train acc 0.644020607826811\n","epoch 3 batch id 1401 loss 0.5863918662071228 train acc 0.6477962169878658\n","epoch 3 train acc 0.6495224204987416\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6335203807f644429ce2d51dee66589b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/365 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 3 test acc 0.594924168297456\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f519bd180bab4ddab7ae5c9c1cd410fb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1457 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 4 batch id 1 loss 0.9479301571846008 train acc 0.65625\n","epoch 4 batch id 201 loss 1.2953827381134033 train acc 0.6764614427860697\n","epoch 4 batch id 401 loss 0.7716375589370728 train acc 0.6931889027431422\n","epoch 4 batch id 601 loss 0.9818992018699646 train acc 0.6979513311148087\n","epoch 4 batch id 801 loss 0.9610339999198914 train acc 0.7094647315855181\n","epoch 4 batch id 1001 loss 0.8501701354980469 train acc 0.7148476523476524\n","epoch 4 batch id 1201 loss 1.1831800937652588 train acc 0.7172928809325562\n","epoch 4 batch id 1401 loss 0.5624518394470215 train acc 0.7204452177016417\n","epoch 4 train acc 0.7217241477922671\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"827184966304434ca7348332c89d37ee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/365 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 4 test acc 0.5960738747553815\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d10724708edf46cab0cc319874c86b51","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1457 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 5 batch id 1 loss 0.8474833965301514 train acc 0.75\n","epoch 5 batch id 201 loss 1.1202601194381714 train acc 0.7366293532338308\n","epoch 5 batch id 401 loss 0.7263317704200745 train acc 0.7520261845386533\n","epoch 5 batch id 601 loss 0.7553805708885193 train acc 0.7566035773710482\n","epoch 5 batch id 801 loss 0.7687376737594604 train acc 0.7625624219725343\n","epoch 5 batch id 1001 loss 0.8911058306694031 train acc 0.7657030469530469\n","epoch 5 batch id 1201 loss 1.0076465606689453 train acc 0.7654818900915903\n","epoch 5 batch id 1401 loss 0.5187050104141235 train acc 0.7662607066381156\n","epoch 5 train acc 0.7671227979867308\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b8dbc80be4c48de8e4c5691fcd9c7c5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/365 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 5 test acc 0.5915362035225049\n"]}],"source":["for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):   # 아까 만든 테스트 배치 데이터 - 정확도 측정\n","\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"]},{"cell_type":"markdown","metadata":{"id":"KPOETSZ2gRQd"},"source":["**모델 정보**"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"-rlHSwCpgRQd"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class '__main__.BERTClassifier'>\n","\n","BERTClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=6, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n"]}],"source":["# 모델 정보\n","print(type(model), model, sep=\"\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["학습 모델 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xi3DiP0gRQd"},"outputs":[],"source":["# 코랩\n","\n","# torch.save(model, './model/sa-tutorial1.pt')\n","# torch.save(model.state_dict(), './model/sa-tutorial1_StateDict.pt')\n","\n","torch.save(model, f'/content/drive/MyDrive/Github/sentiment-analysis/model/sa-tutorial1.pt')\n","torch.save(model.state_dict(), f'/content/drive/MyDrive/Github/sentiment-analysis/model/sa-tutorial1_StateDict.pt')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# 로컬 \n","torch.save(model, 'model/kobert-v1.pt')"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"QYE6sDJogRQd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 351.79 MB\n"]}],"source":["# 모델 사이즈 확인\n","import os\n","\n","model_path1 = 'model/kobert-v1.pt'\n","\n","size1 = os.path.getsize(model_path1) / (1024*1024) # mb 단위\n","#size2 = os.path.getsize(model_path2) / (1024*1024) \n","print(f\"Model size: {size1:.2f} MB\")\n","#print(f\"Model_StateDict size: {size2:.2f} MB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Y_fuPhBwOcg"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.15 ('kobert0')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a424f62477de2ca37995ca837d4a36aa193720671fb7083e9553e863c62eb10b"}}},"nbformat":4,"nbformat_minor":0}
